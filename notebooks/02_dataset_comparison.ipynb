{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Comparison: Competition vs Original Hill of Towie (2016-2020)\n",
    "\n",
    "## Summary\n",
    "\n",
    "This notebook provides a comparison between:\n",
    "1. **Competition Dataset**: Pre-processed Kaggle competition files (2016-2020)\n",
    "2. **Original Dataset**: Raw Hill of Towie data from Zenodo (2016-2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ Path Configuration:\n",
      "   Project root: /home/boujuan/Coding/hill-of-towie-wind-turbine\n",
      "   Competition data: /home/boujuan/Coding/hill-of-towie-wind-turbine/data\n",
      "   Original data: /home/boujuan/Coding/hill-of-towie-wind-turbine/data/external/hill-of-towie-original\n",
      "   Extracted data: /home/boujuan/Coding/hill-of-towie-wind-turbine/data/external/hill-of-towie-original/extracted\n",
      "\n",
      "âœ… Path exists check:\n",
      "   Training data: True\n",
      "   Test data: True\n",
      "   Original data dir: True\n",
      "   Extracted data: True\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure visualization\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Setup paths\n",
    "PROJECT_ROOT = Path('../').resolve()\n",
    "DATA_DIR = PROJECT_ROOT / 'data'\n",
    "ORIGINAL_DATA_DIR = DATA_DIR / 'external' / 'hill-of-towie-original'\n",
    "EXTRACT_DIR = ORIGINAL_DATA_DIR / 'extracted'\n",
    "REPO_DIR = DATA_DIR / 'external' / 'hill-of-towie-repo'\n",
    "\n",
    "# Competition data paths\n",
    "TRAIN_PATH = DATA_DIR / 'train' / 'training_dataset.parquet'\n",
    "TEST_PATH = DATA_DIR / 'test' / 'submission_dataset.parquet'\n",
    "SAMPLE_SUB_PATH = DATA_DIR / 'output' / 'sample_model_submission.csv'\n",
    "\n",
    "print(\"ðŸ“ Path Configuration:\")\n",
    "print(f\"   Project root: {PROJECT_ROOT}\")\n",
    "print(f\"   Competition data: {DATA_DIR}\")\n",
    "print(f\"   Original data: {ORIGINAL_DATA_DIR}\")\n",
    "print(f\"   Extracted data: {EXTRACT_DIR}\")\n",
    "print(f\"\\nâœ… Path exists check:\")\n",
    "print(f\"   Training data: {TRAIN_PATH.exists()}\")\n",
    "print(f\"   Test data: {TEST_PATH.exists()}\")\n",
    "print(f\"   Original data dir: {ORIGINAL_DATA_DIR.exists()}\")\n",
    "print(f\"   Extracted data: {EXTRACT_DIR.exists()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Competition Dataset Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ† COMPETITION DATASET OVERVIEW\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š Dataset Shapes:\n",
      "   Training: (210384, 189) (210,384 rows Ã— 189 columns)\n",
      "   Test: (52704, 159) (52,704 rows Ã— 159 columns)\n",
      "   Sample submission: (52704, 2)\n",
      "\n",
      "ðŸ“… Temporal Coverage:\n",
      "   Training period: 2016-01-01 00:00:00+00:00 to 2019-12-31 23:50:00+00:00\n",
      "   Test period: 2020-01-01 00:00:00+00:00 to 2020-12-31 23:50:00+00:00\n",
      "   Training duration: 1460 days (~4.0 years)\n",
      "   Test duration: 365 days (~1.0 years)\n",
      "\n",
      "â±ï¸ Data Frequency:\n",
      "   Sampling interval: 0 days 00:10:00\n",
      "   Records per day: 144\n",
      "   Expected annual records: 52,560\n"
     ]
    }
   ],
   "source": [
    "# Load competition data\n",
    "train_df = pd.read_parquet(TRAIN_PATH)\n",
    "test_df = pd.read_parquet(TEST_PATH)\n",
    "sample_sub = pd.read_csv(SAMPLE_SUB_PATH)\n",
    "\n",
    "print(\"ðŸ† COMPETITION DATASET OVERVIEW\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Basic statistics\n",
    "print(f\"\\nðŸ“Š Dataset Shapes:\")\n",
    "print(f\"   Training: {train_df.shape} ({train_df.shape[0]:,} rows Ã— {train_df.shape[1]} columns)\")\n",
    "print(f\"   Test: {test_df.shape} ({test_df.shape[0]:,} rows Ã— {test_df.shape[1]} columns)\")\n",
    "print(f\"   Sample submission: {sample_sub.shape}\")\n",
    "\n",
    "# Temporal coverage\n",
    "print(f\"\\nðŸ“… Temporal Coverage:\")\n",
    "print(f\"   Training period: {train_df['TimeStamp_StartFormat'].min()} to {train_df['TimeStamp_StartFormat'].max()}\")\n",
    "print(f\"   Test period: {test_df['TimeStamp_StartFormat'].min()} to {test_df['TimeStamp_StartFormat'].max()}\")\n",
    "\n",
    "# Calculate actual time spans\n",
    "train_days = (train_df['TimeStamp_StartFormat'].max() - train_df['TimeStamp_StartFormat'].min()).days\n",
    "test_days = (test_df['TimeStamp_StartFormat'].max() - test_df['TimeStamp_StartFormat'].min()).days\n",
    "print(f\"   Training duration: {train_days} days (~{train_days/365:.1f} years)\")\n",
    "print(f\"   Test duration: {test_days} days (~{test_days/365:.1f} years)\")\n",
    "\n",
    "# Data frequency\n",
    "time_diffs = train_df['TimeStamp_StartFormat'].diff().dropna()\n",
    "print(f\"\\nâ±ï¸ Data Frequency:\")\n",
    "print(f\"   Sampling interval: {time_diffs.mode()[0]}\")\n",
    "print(f\"   Records per day: {24 * 60 / 10:.0f}\")\n",
    "print(f\"   Expected annual records: {365 * 24 * 6:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŒ¬ï¸ TURBINE ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "ðŸ”§ Turbines in Competition Data:\n",
      "   Available: [1, 2, 3, 4, 5, 7]\n",
      "   Missing: Turbine 6 (gap between 5 and 7)\n",
      "   Total: 6 turbines\n",
      "\n",
      "ðŸ“‹ Fields per Turbine: 29\n",
      "\n",
      "   SCADA Fields:\n",
      "      1. wtc_AcWindSp_mean\n",
      "      2. wtc_AcWindSp_min\n",
      "      3. wtc_AcWindSp_max\n",
      "      4. wtc_AcWindSp_stddev\n",
      "      5. wtc_ScYawPos_mean\n",
      "      6. wtc_ScYawPos_min\n",
      "      7. wtc_ScYawPos_max\n",
      "      8. wtc_ScYawPos_stddev\n",
      "      9. wtc_NacelPos_mean\n",
      "     10. wtc_NacelPos_min\n",
      "     ... and 18 more SCADA fields\n",
      "\n",
      "   Weather Fields (ERA5):\n"
     ]
    }
   ],
   "source": [
    "# Analyze turbine coverage\n",
    "print(\"ðŸŒ¬ï¸ TURBINE ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Extract turbine information from column names\n",
    "turbine_cols = [col for col in train_df.columns if ';' in col]\n",
    "turbine_fields = {}\n",
    "\n",
    "for col in turbine_cols:\n",
    "    parts = col.split(';')\n",
    "    if len(parts) == 2 and parts[1].isdigit():\n",
    "        field, turbine = parts\n",
    "        turbine_id = int(turbine)\n",
    "        if turbine_id not in turbine_fields:\n",
    "            turbine_fields[turbine_id] = []\n",
    "        turbine_fields[turbine_id].append(field)\n",
    "\n",
    "competition_turbines = sorted(turbine_fields.keys())\n",
    "print(f\"\\nðŸ”§ Turbines in Competition Data:\")\n",
    "print(f\"   Available: {competition_turbines}\")\n",
    "print(f\"   Missing: Turbine 6 (gap between 5 and 7)\")\n",
    "print(f\"   Total: {len(competition_turbines)} turbines\")\n",
    "\n",
    "# Fields per turbine\n",
    "if competition_turbines:\n",
    "    sample_turbine = competition_turbines[0]\n",
    "    fields = turbine_fields[sample_turbine]\n",
    "    print(f\"\\nðŸ“‹ Fields per Turbine: {len(fields)}\")\n",
    "    print(\"\\n   SCADA Fields:\")\n",
    "    scada_fields = [f for f in fields if not f.startswith('ERA5_') and f != 'ShutdownDuration']\n",
    "    for i, field in enumerate(scada_fields[:10], 1):\n",
    "        print(f\"     {i:2}. {field}\")\n",
    "    if len(scada_fields) > 10:\n",
    "        print(f\"     ... and {len(scada_fields)-10} more SCADA fields\")\n",
    "    \n",
    "    print(\"\\n   Weather Fields (ERA5):\")\n",
    "    weather_fields = [f for f in fields if f.startswith('ERA5_')]\n",
    "    for i, field in enumerate(weather_fields[:5], 1):\n",
    "        print(f\"     {i:2}. {field}\")\n",
    "    if len(weather_fields) > 5:\n",
    "        print(f\"     ... and {len(weather_fields)-5} more ERA5 fields\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¯ TARGET & VALIDATION ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š Target Variable:\n",
      "   Column name: 'target'\n",
      "   Definition: Active power of Turbine 1 (clipped at 0)\n",
      "   Statistics:\n",
      "     - Mean: 644.15 kW\n",
      "     - Std: 713.70 kW\n",
      "     - Min: 0.00 kW\n",
      "     - Max: 2304.95 kW\n",
      "     - Zero values: 32,328 (15.4%)\n",
      "\n",
      "âœ… Validation Flag (is_valid):\n",
      "   Purpose: Only valid periods count for competition scoring\n",
      "   Valid records: 201,323 (95.7%)\n",
      "   Invalid records: 9,061 (4.3%)\n",
      "\n",
      "   Validity Conditions (all must be true):\n",
      "     1. ShutdownDuration;1 == 0 (turbine not shut down)\n",
      "     2. wtc_ScReToOp_timeon;1 == 600 (full 10-min operation)\n",
      "     3. wtc_ActPower_mean;1 is not null\n",
      "\n",
      "   Condition Breakdown:\n",
      "     - No shutdown: 202,959 records\n",
      "     - Full operation: 201,386 records\n",
      "     - Power not null: 207,924 records\n"
     ]
    }
   ],
   "source": [
    "# Analyze target and validation\n",
    "print(\"ðŸŽ¯ TARGET & VALIDATION ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Target column\n",
    "print(f\"\\nðŸ“Š Target Variable:\")\n",
    "print(f\"   Column name: 'target'\")\n",
    "print(f\"   Definition: Active power of Turbine 1 (clipped at 0)\")\n",
    "print(f\"   Statistics:\")\n",
    "print(f\"     - Mean: {train_df['target'].mean():.2f} kW\")\n",
    "print(f\"     - Std: {train_df['target'].std():.2f} kW\")\n",
    "print(f\"     - Min: {train_df['target'].min():.2f} kW\")\n",
    "print(f\"     - Max: {train_df['target'].max():.2f} kW\")\n",
    "print(f\"     - Zero values: {(train_df['target'] == 0).sum():,} ({(train_df['target'] == 0).mean()*100:.1f}%)\")\n",
    "\n",
    "# Validation flag\n",
    "print(f\"\\nâœ… Validation Flag (is_valid):\")\n",
    "print(f\"   Purpose: Only valid periods count for competition scoring\")\n",
    "print(f\"   Valid records: {train_df['is_valid'].sum():,} ({train_df['is_valid'].mean()*100:.1f}%)\")\n",
    "print(f\"   Invalid records: {(~train_df['is_valid']).sum():,} ({(~train_df['is_valid']).mean()*100:.1f}%)\")\n",
    "\n",
    "# is_valid conditions\n",
    "print(f\"\\n   Validity Conditions (all must be true):\")\n",
    "print(f\"     1. ShutdownDuration;1 == 0 (turbine not shut down)\")\n",
    "print(f\"     2. wtc_ScReToOp_timeon;1 == 600 (full 10-min operation)\")\n",
    "print(f\"     3. wtc_ActPower_mean;1 is not null\")\n",
    "\n",
    "# Check validity conditions\n",
    "if 'ShutdownDuration;1' in train_df.columns:\n",
    "    shutdown_zero = (train_df['ShutdownDuration;1'] == 0).sum()\n",
    "    full_operation = (train_df['wtc_ScReToOp_timeon;1'] == 600).sum()\n",
    "    power_not_null = train_df['wtc_ActPower_mean;1'].notna().sum()\n",
    "    \n",
    "    print(f\"\\n   Condition Breakdown:\")\n",
    "    print(f\"     - No shutdown: {shutdown_zero:,} records\")\n",
    "    print(f\"     - Full operation: {full_operation:,} records\")\n",
    "    print(f\"     - Power not null: {power_not_null:,} records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Original Dataset Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¦ ORIGINAL DATASET OVERVIEW\n",
      "================================================================================\n",
      "\n",
      "ðŸ“… Years Available: 2016, 2017, 2018, 2019, 2020\n",
      "\n",
      "   ðŸ“ Year 2016:\n",
      "      Files: 156\n",
      "      Size: 6.27 GB\n",
      "      Tables: tblAlarmLog, tblDailySummary, tblGridScientific, tblGrid, tblSCTurbine, tblSCTurCount, tblSCTurDigiIn, tblSCTurDigiOut, tblSCTurFlag, tblSCTurGrid, tblSCTurIntern, tblSCTurPress, tblSCTurTemp\n",
      "\n",
      "   ðŸ“ Year 2017:\n",
      "      Files: 156\n",
      "      Size: 5.49 GB\n",
      "      Tables: tblAlarmLog, tblDailySummary, tblGridScientific, tblGrid, tblSCTurbine, tblSCTurCount, tblSCTurDigiIn, tblSCTurDigiOut, tblSCTurFlag, tblSCTurGrid, tblSCTurIntern, tblSCTurPress, tblSCTurTemp\n",
      "\n",
      "   ðŸ“ Year 2018:\n",
      "      Files: 156\n",
      "      Size: 5.59 GB\n",
      "      Tables: tblAlarmLog, tblDailySummary, tblGridScientific, tblGrid, tblSCTurbine, tblSCTurCount, tblSCTurDigiIn, tblSCTurDigiOut, tblSCTurFlag, tblSCTurGrid, tblSCTurIntern, tblSCTurPress, tblSCTurTemp\n",
      "\n",
      "   ðŸ“ Year 2019:\n",
      "      Files: 156\n",
      "      Size: 5.58 GB\n",
      "      Tables: tblAlarmLog, tblDailySummary, tblGridScientific, tblGrid, tblSCTurbine, tblSCTurCount, tblSCTurDigiIn, tblSCTurDigiOut, tblSCTurFlag, tblSCTurGrid, tblSCTurIntern, tblSCTurPress, tblSCTurTemp\n",
      "\n",
      "   ðŸ“ Year 2020:\n",
      "      Files: 156\n",
      "      Size: 5.56 GB\n",
      "      Tables: tblAlarmLog, tblDailySummary, tblGridScientific, tblGrid, tblSCTurbine, tblSCTurCount, tblSCTurDigiIn, tblSCTurDigiOut, tblSCTurFlag, tblSCTurGrid, tblSCTurIntern, tblSCTurPress, tblSCTurTemp\n",
      "\n",
      "ðŸ“š GitHub Repository: âœ… Available at /home/boujuan/Coding/hill-of-towie-wind-turbine/data/external/hill-of-towie-repo\n",
      "   Processing scripts found: 5\n"
     ]
    }
   ],
   "source": [
    "print(\"ðŸ“¦ ORIGINAL DATASET OVERVIEW\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Check download status\n",
    "available_years = []\n",
    "year_info = {}\n",
    "\n",
    "if EXTRACT_DIR.exists():\n",
    "    year_dirs = sorted([d for d in EXTRACT_DIR.iterdir() if d.is_dir()])\n",
    "    available_years = [d.name for d in year_dirs]\n",
    "    \n",
    "    print(f\"\\nðŸ“… Years Available: {', '.join(available_years) if available_years else 'None - download in progress'}\")\n",
    "    \n",
    "    # Analyze each year\n",
    "    for year_dir in year_dirs:\n",
    "        year = year_dir.name\n",
    "        csv_files = list(year_dir.glob('*.csv'))\n",
    "        total_size = sum(f.stat().st_size for f in csv_files) / (1024**3)  # GB\n",
    "        \n",
    "        # Group by table type\n",
    "        tables = {}\n",
    "        for f in csv_files:\n",
    "            table_name = f.name.split('_')[0] if '_' in f.name else f.stem\n",
    "            if table_name not in tables:\n",
    "                tables[table_name] = []\n",
    "            tables[table_name].append(f)\n",
    "        \n",
    "        year_info[year] = {\n",
    "            'files': len(csv_files),\n",
    "            'size_gb': total_size,\n",
    "            'tables': list(tables.keys())\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n   ðŸ“ Year {year}:\")\n",
    "        print(f\"      Files: {len(csv_files)}\")\n",
    "        print(f\"      Size: {total_size:.2f} GB\")\n",
    "        print(f\"      Tables: {', '.join(tables.keys())}\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸ Original data not yet extracted. Run download_original_data.py first.\")\n",
    "\n",
    "# Check GitHub repo for additional context\n",
    "if REPO_DIR.exists():\n",
    "    print(f\"\\nðŸ“š GitHub Repository: âœ… Available at {REPO_DIR}\")\n",
    "    # Check for useful scripts\n",
    "    scripts_dir = REPO_DIR / 'scripts'\n",
    "    if scripts_dir.exists():\n",
    "        py_files = list(scripts_dir.rglob('*.py'))\n",
    "        print(f\"   Processing scripts found: {len(py_files)}\")\n",
    "else:\n",
    "    print(f\"\\nðŸ“š GitHub Repository: âŒ Not cloned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” ORIGINAL DATA STRUCTURE ANALYSIS (2016 Sample)\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š tblSCTurbine:\n",
      "   Shape: (1000, 85)\n",
      "   Columns: 85\n",
      "   Turbines in sample: [np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6), np.int64(7), np.int64(8), np.int64(9), np.int64(10)]...\n",
      "   ðŸŽ¯ Turbine 6 (StationId 2304515): âœ… Present\n",
      "   Key columns available:\n",
      "     âœ“ wtc_AcWindSp_mean\n",
      "     âœ“ wtc_ScYawPos_mean\n",
      "     âœ“ wtc_NacelPos_mean\n",
      "     âœ“ wtc_GenRpm_mean\n",
      "     âœ“ wtc_PitcPosA_mean\n",
      "     âœ“ wtc_PitcPosB_mean\n",
      "     âœ“ wtc_PitcPosC_mean\n",
      "\n",
      "ðŸ“Š tblSCTurGrid:\n",
      "   Shape: (1000, 52)\n",
      "   Columns: 52\n",
      "   Turbines in sample: [np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6), np.int64(7), np.int64(8), np.int64(9), np.int64(10)]...\n",
      "   ðŸŽ¯ Turbine 6 (StationId 2304515): âœ… Present\n",
      "   âœ… Contains active power (target variable source)\n",
      "\n",
      "ðŸ“Š tblSCTurTemp:\n",
      "   Shape: (1000, 142)\n",
      "   Columns: 142\n",
      "   Turbines in sample: [np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6), np.int64(7), np.int64(8), np.int64(9), np.int64(10)]...\n",
      "   ðŸŽ¯ Turbine 6 (StationId 2304515): âœ… Present\n",
      "\n",
      "ðŸ“Š tblSCTurFlag:\n",
      "   Shape: (1000, 46)\n",
      "   Columns: 46\n",
      "   Turbines in sample: [np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6), np.int64(7), np.int64(8), np.int64(9), np.int64(10)]...\n",
      "   ðŸŽ¯ Turbine 6 (StationId 2304515): âœ… Present\n"
     ]
    }
   ],
   "source": [
    "# Deep dive into data structure if available\n",
    "if available_years and '2016' in available_years:\n",
    "    print(\"ðŸ” ORIGINAL DATA STRUCTURE ANALYSIS (2016 Sample)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    year_dir = EXTRACT_DIR / '2016'\n",
    "    \n",
    "    # Analyze each table type\n",
    "    table_samples = {}\n",
    "    \n",
    "    for table_type in ['tblSCTurbine', 'tblSCTurGrid', 'tblSCTurTemp', 'tblSCTurFlag']:\n",
    "        files = list(year_dir.glob(f'{table_type}*.csv'))\n",
    "        if files:\n",
    "            print(f\"\\nðŸ“Š {table_type}:\")\n",
    "            # Read sample\n",
    "            df_sample = pd.read_csv(files[0], nrows=1000)\n",
    "            table_samples[table_type] = df_sample\n",
    "            \n",
    "            print(f\"   Shape: {df_sample.shape}\")\n",
    "            print(f\"   Columns: {len(df_sample.columns)}\")\n",
    "            \n",
    "            # Check for StationId (turbine identifier)\n",
    "            if 'StationId' in df_sample.columns:\n",
    "                unique_stations = df_sample['StationId'].unique()\n",
    "                turbine_ids = sorted([sid - 2304509 for sid in unique_stations])\n",
    "                print(f\"   Turbines in sample: {turbine_ids[:10]}...\" if len(turbine_ids) > 10 else f\"   Turbines: {turbine_ids}\")\n",
    "                \n",
    "                # Check for Turbine 6\n",
    "                turbine_6_station = 2304515\n",
    "                has_turbine_6 = turbine_6_station in unique_stations\n",
    "                print(f\"   ðŸŽ¯ Turbine 6 (StationId {turbine_6_station}): {'âœ… Present' if has_turbine_6 else 'âŒ Not in sample'}\")\n",
    "            \n",
    "            # Show key columns\n",
    "            if table_type == 'tblSCTurbine':\n",
    "                key_cols = ['wtc_AcWindSp_mean', 'wtc_ScYawPos_mean', 'wtc_NacelPos_mean', \n",
    "                           'wtc_GenRpm_mean', 'wtc_PitcPosA_mean', 'wtc_PitcPosB_mean', 'wtc_PitcPosC_mean']\n",
    "                print(\"   Key columns available:\")\n",
    "                for col in key_cols:\n",
    "                    if col in df_sample.columns:\n",
    "                        print(f\"     âœ“ {col}\")\n",
    "                    else:\n",
    "                        print(f\"     âœ— {col}\")\n",
    "            \n",
    "            elif table_type == 'tblSCTurGrid':\n",
    "                if 'wtc_ActPower_mean' in df_sample.columns:\n",
    "                    print(f\"   âœ… Contains active power (target variable source)\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸ Cannot analyze structure - data not yet available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š FEATURE COMPARISON\n",
      "================================================================================\n",
      "\n",
      "ðŸ† Competition Dataset Features:\n",
      "   Total unique features per turbine: 29\n",
      "\n",
      "   Feature Categories:\n",
      "     SCADA: 28 features\n",
      "     Weather (ERA5): 0 features\n",
      "     Operational: 1 features\n",
      "\n",
      "ðŸ“¦ Original Dataset Features:\n",
      "   Total unique features: 317\n",
      "\n",
      "   Feature Overlap:\n",
      "     Matching features: 28/28\n",
      "     Coverage: 100.0%\n",
      "     Additional original features: 289\n",
      "\n",
      "   Unique to Original (sample):\n",
      "      1. wtc_HydOilTm_max\n",
      "      2. wtc_TowerFrq_Frequenc_max\n",
      "      3. wtc_A3LefTmp_stddev\n",
      "      4. wtc_IMSRotTm_stddev\n",
      "      5. wtc_HSGenTmp_max\n",
      "      6. wtc_PitcRefA_stddev\n",
      "      7. wtc_TetAnFrq_stddev\n",
      "      8. wtc_RawPower_endvalue\n",
      "      9. wtc_GeOilTmp_max\n",
      "     10. wtc_ScInOper_counts\n",
      "     ... and 279 more\n"
     ]
    }
   ],
   "source": [
    "print(\"ðŸ“Š FEATURE COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Competition features (excluding turbine ID)\n",
    "comp_features = [col.split(';')[0] for col in train_df.columns if ';1' in col]\n",
    "comp_features_unique = list(dict.fromkeys(comp_features))  # Preserve order, remove duplicates\n",
    "\n",
    "print(f\"\\nðŸ† Competition Dataset Features:\")\n",
    "print(f\"   Total unique features per turbine: {len(comp_features_unique)}\")\n",
    "\n",
    "# Categorize features\n",
    "scada_features = [f for f in comp_features_unique if not f.startswith('ERA5_') and f != 'ShutdownDuration']\n",
    "weather_features = [f for f in comp_features_unique if f.startswith('ERA5_')]\n",
    "operational_features = ['ShutdownDuration'] if 'ShutdownDuration' in comp_features_unique else []\n",
    "\n",
    "print(f\"\\n   Feature Categories:\")\n",
    "print(f\"     SCADA: {len(scada_features)} features\")\n",
    "print(f\"     Weather (ERA5): {len(weather_features)} features\")\n",
    "print(f\"     Operational: {len(operational_features)} features\")\n",
    "\n",
    "# If original data is available, compare\n",
    "if available_years and 'table_samples' in locals():\n",
    "    print(f\"\\nðŸ“¦ Original Dataset Features:\")\n",
    "    \n",
    "    all_original_cols = set()\n",
    "    for table_name, df in table_samples.items():\n",
    "        cols = set(df.columns) - {'TimeStamp', 'StationId'}\n",
    "        all_original_cols.update(cols)\n",
    "    \n",
    "    print(f\"   Total unique features: {len(all_original_cols)}\")\n",
    "    \n",
    "    # Find matching features\n",
    "    matches = []\n",
    "    for comp_feat in scada_features:\n",
    "        for orig_feat in all_original_cols:\n",
    "            if comp_feat in orig_feat or orig_feat in comp_feat:\n",
    "                matches.append((comp_feat, orig_feat))\n",
    "                break\n",
    "    \n",
    "    print(f\"\\n   Feature Overlap:\")\n",
    "    print(f\"     Matching features: {len(matches)}/{len(scada_features)}\")\n",
    "    print(f\"     Coverage: {len(matches)/len(scada_features)*100:.1f}%\")\n",
    "    print(f\"     Additional original features: {len(all_original_cols) - len(matches)}\")\n",
    "    \n",
    "    # Show unique original features\n",
    "    print(f\"\\n   Unique to Original (sample):\")\n",
    "    unique_original = all_original_cols - set([m[1] for m in matches])\n",
    "    for i, feat in enumerate(list(unique_original)[:10], 1):\n",
    "        print(f\"     {i:2}. {feat}\")\n",
    "    if len(unique_original) > 10:\n",
    "        print(f\"     ... and {len(unique_original)-10} more\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hot-kaggle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
