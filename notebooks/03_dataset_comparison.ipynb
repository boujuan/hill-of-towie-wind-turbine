{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comprehensive Dataset Comparison: Competition vs Original Hill of Towie (2016-2020)\n",
    "\n",
    "## Executive Summary\n",
    "\n",
    "This notebook provides an in-depth comparison between:\n",
    "1. **Competition Dataset**: Pre-processed Kaggle competition files (2016-2020)\n",
    "2. **Original Dataset**: Raw Hill of Towie data from Zenodo (2016-2020)\n",
    "\n",
    "### Key Questions Addressed:\n",
    "- What additional value does the original dataset provide?\n",
    "- Is Turbine 6 data extraction worth the effort?\n",
    "- What are the data leakage risks?\n",
    "- What is the true complexity of integrating original data?\n",
    "- Should we use the original data or focus on competition data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Path Configuration:\n",
      "   Project root: /home/boujuan/Coding/hill-of-towie-wind-turbine\n",
      "   Competition data: /home/boujuan/Coding/hill-of-towie-wind-turbine/data\n",
      "   Original data: /home/boujuan/Coding/hill-of-towie-wind-turbine/data/external/hill-of-towie-original\n",
      "   Extracted data: /home/boujuan/Coding/hill-of-towie-wind-turbine/data/external/hill-of-towie-original/extracted\n",
      "\n",
      "‚úÖ Path exists check:\n",
      "   Training data: True\n",
      "   Test data: True\n",
      "   Original data dir: True\n",
      "   Extracted data: True\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import json\n",
    "import zipfile\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure visualization\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Setup paths\n",
    "PROJECT_ROOT = Path('../').resolve()\n",
    "DATA_DIR = PROJECT_ROOT / 'data'\n",
    "ORIGINAL_DATA_DIR = DATA_DIR / 'external' / 'hill-of-towie-original'\n",
    "EXTRACT_DIR = ORIGINAL_DATA_DIR / 'extracted'\n",
    "REPO_DIR = DATA_DIR / 'external' / 'hill-of-towie-repo'\n",
    "\n",
    "# Competition data paths\n",
    "TRAIN_PATH = DATA_DIR / 'train' / 'training_dataset.parquet'\n",
    "TEST_PATH = DATA_DIR / 'test' / 'submission_dataset.parquet'\n",
    "SAMPLE_SUB_PATH = DATA_DIR / 'raw' / 'sample_model_submission.csv'\n",
    "\n",
    "print(\"üìÅ Path Configuration:\")\n",
    "print(f\"   Project root: {PROJECT_ROOT}\")\n",
    "print(f\"   Competition data: {DATA_DIR}\")\n",
    "print(f\"   Original data: {ORIGINAL_DATA_DIR}\")\n",
    "print(f\"   Extracted data: {EXTRACT_DIR}\")\n",
    "print(f\"\\n‚úÖ Path exists check:\")\n",
    "print(f\"   Training data: {TRAIN_PATH.exists()}\")\n",
    "print(f\"   Test data: {TEST_PATH.exists()}\")\n",
    "print(f\"   Original data dir: {ORIGINAL_DATA_DIR.exists()}\")\n",
    "print(f\"   Extracted data: {EXTRACT_DIR.exists()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Competition Dataset Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÜ COMPETITION DATASET OVERVIEW\n",
      "================================================================================\n",
      "\n",
      "üìä Dataset Shapes:\n",
      "   Training: (210384, 189) (210,384 rows √ó 189 columns)\n",
      "   Test: (52704, 159) (52,704 rows √ó 159 columns)\n",
      "   Sample submission: (52704, 2)\n",
      "\n",
      "üìÖ Temporal Coverage:\n",
      "   Training period: 2016-01-01 00:00:00+00:00 to 2019-12-31 23:50:00+00:00\n",
      "   Test period: 2020-01-01 00:00:00+00:00 to 2020-12-31 23:50:00+00:00\n",
      "   Training duration: 1460 days (~4.0 years)\n",
      "   Test duration: 365 days (~1.0 years)\n",
      "\n",
      "‚è±Ô∏è Data Frequency:\n",
      "   Sampling interval: 0 days 00:10:00\n",
      "   Records per day: 144\n",
      "   Expected annual records: 52,560\n"
     ]
    }
   ],
   "source": [
    "# Load competition data\n",
    "train_df = pd.read_parquet(TRAIN_PATH)\n",
    "test_df = pd.read_parquet(TEST_PATH)\n",
    "sample_sub = pd.read_csv(SAMPLE_SUB_PATH)\n",
    "\n",
    "print(\"üèÜ COMPETITION DATASET OVERVIEW\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Basic statistics\n",
    "print(f\"\\nüìä Dataset Shapes:\")\n",
    "print(f\"   Training: {train_df.shape} ({train_df.shape[0]:,} rows √ó {train_df.shape[1]} columns)\")\n",
    "print(f\"   Test: {test_df.shape} ({test_df.shape[0]:,} rows √ó {test_df.shape[1]} columns)\")\n",
    "print(f\"   Sample submission: {sample_sub.shape}\")\n",
    "\n",
    "# Temporal coverage\n",
    "print(f\"\\nüìÖ Temporal Coverage:\")\n",
    "print(f\"   Training period: {train_df['TimeStamp_StartFormat'].min()} to {train_df['TimeStamp_StartFormat'].max()}\")\n",
    "print(f\"   Test period: {test_df['TimeStamp_StartFormat'].min()} to {test_df['TimeStamp_StartFormat'].max()}\")\n",
    "\n",
    "# Calculate actual time spans\n",
    "train_days = (train_df['TimeStamp_StartFormat'].max() - train_df['TimeStamp_StartFormat'].min()).days\n",
    "test_days = (test_df['TimeStamp_StartFormat'].max() - test_df['TimeStamp_StartFormat'].min()).days\n",
    "print(f\"   Training duration: {train_days} days (~{train_days/365:.1f} years)\")\n",
    "print(f\"   Test duration: {test_days} days (~{test_days/365:.1f} years)\")\n",
    "\n",
    "# Data frequency\n",
    "time_diffs = train_df['TimeStamp_StartFormat'].diff().dropna()\n",
    "print(f\"\\n‚è±Ô∏è Data Frequency:\")\n",
    "print(f\"   Sampling interval: {time_diffs.mode()[0]}\")\n",
    "print(f\"   Records per day: {24 * 60 / 10:.0f}\")\n",
    "print(f\"   Expected annual records: {365 * 24 * 6:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üå¨Ô∏è TURBINE ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "üîß Turbines in Competition Data:\n",
      "   Available: [1, 2, 3, 4, 5, 7]\n",
      "   Missing: Turbine 6 (gap between 5 and 7)\n",
      "   Total: 6 turbines\n",
      "\n",
      "üìã Fields per Turbine: 29\n",
      "\n",
      "   SCADA Fields:\n",
      "      1. wtc_AcWindSp_mean\n",
      "      2. wtc_AcWindSp_min\n",
      "      3. wtc_AcWindSp_max\n",
      "      4. wtc_AcWindSp_stddev\n",
      "      5. wtc_ScYawPos_mean\n",
      "      6. wtc_ScYawPos_min\n",
      "      7. wtc_ScYawPos_max\n",
      "      8. wtc_ScYawPos_stddev\n",
      "      9. wtc_NacelPos_mean\n",
      "     10. wtc_NacelPos_min\n",
      "     ... and 18 more SCADA fields\n",
      "\n",
      "   Weather Fields (ERA5):\n"
     ]
    }
   ],
   "source": [
    "# Analyze turbine coverage\n",
    "print(\"üå¨Ô∏è TURBINE ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Extract turbine information from column names\n",
    "turbine_cols = [col for col in train_df.columns if ';' in col]\n",
    "turbine_fields = {}\n",
    "\n",
    "for col in turbine_cols:\n",
    "    parts = col.split(';')\n",
    "    if len(parts) == 2 and parts[1].isdigit():\n",
    "        field, turbine = parts\n",
    "        turbine_id = int(turbine)\n",
    "        if turbine_id not in turbine_fields:\n",
    "            turbine_fields[turbine_id] = []\n",
    "        turbine_fields[turbine_id].append(field)\n",
    "\n",
    "competition_turbines = sorted(turbine_fields.keys())\n",
    "print(f\"\\nüîß Turbines in Competition Data:\")\n",
    "print(f\"   Available: {competition_turbines}\")\n",
    "print(f\"   Missing: Turbine 6 (gap between 5 and 7)\")\n",
    "print(f\"   Total: {len(competition_turbines)} turbines\")\n",
    "\n",
    "# Fields per turbine\n",
    "if competition_turbines:\n",
    "    sample_turbine = competition_turbines[0]\n",
    "    fields = turbine_fields[sample_turbine]\n",
    "    print(f\"\\nüìã Fields per Turbine: {len(fields)}\")\n",
    "    print(\"\\n   SCADA Fields:\")\n",
    "    scada_fields = [f for f in fields if not f.startswith('ERA5_') and f != 'ShutdownDuration']\n",
    "    for i, field in enumerate(scada_fields[:10], 1):\n",
    "        print(f\"     {i:2}. {field}\")\n",
    "    if len(scada_fields) > 10:\n",
    "        print(f\"     ... and {len(scada_fields)-10} more SCADA fields\")\n",
    "    \n",
    "    print(\"\\n   Weather Fields (ERA5):\")\n",
    "    weather_fields = [f for f in fields if f.startswith('ERA5_')]\n",
    "    for i, field in enumerate(weather_fields[:5], 1):\n",
    "        print(f\"     {i:2}. {field}\")\n",
    "    if len(weather_fields) > 5:\n",
    "        print(f\"     ... and {len(weather_fields)-5} more ERA5 fields\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ TARGET & VALIDATION ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "üìä Target Variable:\n",
      "   Column name: 'target'\n",
      "   Definition: Active power of Turbine 1 (clipped at 0)\n",
      "   Statistics:\n",
      "     - Mean: 644.15 kW\n",
      "     - Std: 713.70 kW\n",
      "     - Min: 0.00 kW\n",
      "     - Max: 2304.95 kW\n",
      "     - Zero values: 32,328 (15.4%)\n",
      "\n",
      "‚úÖ Validation Flag (is_valid):\n",
      "   Purpose: Only valid periods count for competition scoring\n",
      "   Valid records: 201,323 (95.7%)\n",
      "   Invalid records: 9,061 (4.3%)\n",
      "\n",
      "   Validity Conditions (all must be true):\n",
      "     1. ShutdownDuration;1 == 0 (turbine not shut down)\n",
      "     2. wtc_ScReToOp_timeon;1 == 600 (full 10-min operation)\n",
      "     3. wtc_ActPower_mean;1 is not null\n",
      "\n",
      "   Condition Breakdown:\n",
      "     - No shutdown: 202,959 records\n",
      "     - Full operation: 201,386 records\n",
      "     - Power not null: 207,924 records\n"
     ]
    }
   ],
   "source": [
    "# Analyze target and validation\n",
    "print(\"üéØ TARGET & VALIDATION ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Target column\n",
    "print(f\"\\nüìä Target Variable:\")\n",
    "print(f\"   Column name: 'target'\")\n",
    "print(f\"   Definition: Active power of Turbine 1 (clipped at 0)\")\n",
    "print(f\"   Statistics:\")\n",
    "print(f\"     - Mean: {train_df['target'].mean():.2f} kW\")\n",
    "print(f\"     - Std: {train_df['target'].std():.2f} kW\")\n",
    "print(f\"     - Min: {train_df['target'].min():.2f} kW\")\n",
    "print(f\"     - Max: {train_df['target'].max():.2f} kW\")\n",
    "print(f\"     - Zero values: {(train_df['target'] == 0).sum():,} ({(train_df['target'] == 0).mean()*100:.1f}%)\")\n",
    "\n",
    "# Validation flag\n",
    "print(f\"\\n‚úÖ Validation Flag (is_valid):\")\n",
    "print(f\"   Purpose: Only valid periods count for competition scoring\")\n",
    "print(f\"   Valid records: {train_df['is_valid'].sum():,} ({train_df['is_valid'].mean()*100:.1f}%)\")\n",
    "print(f\"   Invalid records: {(~train_df['is_valid']).sum():,} ({(~train_df['is_valid']).mean()*100:.1f}%)\")\n",
    "\n",
    "# is_valid conditions\n",
    "print(f\"\\n   Validity Conditions (all must be true):\")\n",
    "print(f\"     1. ShutdownDuration;1 == 0 (turbine not shut down)\")\n",
    "print(f\"     2. wtc_ScReToOp_timeon;1 == 600 (full 10-min operation)\")\n",
    "print(f\"     3. wtc_ActPower_mean;1 is not null\")\n",
    "\n",
    "# Check validity conditions\n",
    "if 'ShutdownDuration;1' in train_df.columns:\n",
    "    shutdown_zero = (train_df['ShutdownDuration;1'] == 0).sum()\n",
    "    full_operation = (train_df['wtc_ScReToOp_timeon;1'] == 600).sum()\n",
    "    power_not_null = train_df['wtc_ActPower_mean;1'].notna().sum()\n",
    "    \n",
    "    print(f\"\\n   Condition Breakdown:\")\n",
    "    print(f\"     - No shutdown: {shutdown_zero:,} records\")\n",
    "    print(f\"     - Full operation: {full_operation:,} records\")\n",
    "    print(f\"     - Power not null: {power_not_null:,} records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Original Dataset Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ ORIGINAL DATASET OVERVIEW\n",
      "================================================================================\n",
      "\n",
      "üìÖ Years Available: 2016, 2017, 2018, 2019, 2020\n",
      "\n",
      "   üìÅ Year 2016:\n",
      "      Files: 156\n",
      "      Size: 6.27 GB\n",
      "      Tables: tblAlarmLog, tblDailySummary, tblGridScientific, tblGrid, tblSCTurbine, tblSCTurCount, tblSCTurDigiIn, tblSCTurDigiOut, tblSCTurFlag, tblSCTurGrid, tblSCTurIntern, tblSCTurPress, tblSCTurTemp\n",
      "\n",
      "   üìÅ Year 2017:\n",
      "      Files: 156\n",
      "      Size: 5.49 GB\n",
      "      Tables: tblAlarmLog, tblDailySummary, tblGridScientific, tblGrid, tblSCTurbine, tblSCTurCount, tblSCTurDigiIn, tblSCTurDigiOut, tblSCTurFlag, tblSCTurGrid, tblSCTurIntern, tblSCTurPress, tblSCTurTemp\n",
      "\n",
      "   üìÅ Year 2018:\n",
      "      Files: 156\n",
      "      Size: 5.59 GB\n",
      "      Tables: tblAlarmLog, tblDailySummary, tblGridScientific, tblGrid, tblSCTurbine, tblSCTurCount, tblSCTurDigiIn, tblSCTurDigiOut, tblSCTurFlag, tblSCTurGrid, tblSCTurIntern, tblSCTurPress, tblSCTurTemp\n",
      "\n",
      "   üìÅ Year 2019:\n",
      "      Files: 156\n",
      "      Size: 5.58 GB\n",
      "      Tables: tblAlarmLog, tblDailySummary, tblGridScientific, tblGrid, tblSCTurbine, tblSCTurCount, tblSCTurDigiIn, tblSCTurDigiOut, tblSCTurFlag, tblSCTurGrid, tblSCTurIntern, tblSCTurPress, tblSCTurTemp\n",
      "\n",
      "   üìÅ Year 2020:\n",
      "      Files: 156\n",
      "      Size: 5.56 GB\n",
      "      Tables: tblAlarmLog, tblDailySummary, tblGridScientific, tblGrid, tblSCTurbine, tblSCTurCount, tblSCTurDigiIn, tblSCTurDigiOut, tblSCTurFlag, tblSCTurGrid, tblSCTurIntern, tblSCTurPress, tblSCTurTemp\n",
      "\n",
      "üìö GitHub Repository: ‚úÖ Available at /home/boujuan/Coding/hill-of-towie-wind-turbine/data/external/hill-of-towie-repo\n",
      "   Processing scripts found: 5\n"
     ]
    }
   ],
   "source": [
    "print(\"üì¶ ORIGINAL DATASET OVERVIEW\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Check download status\n",
    "available_years = []\n",
    "year_info = {}\n",
    "\n",
    "if EXTRACT_DIR.exists():\n",
    "    year_dirs = sorted([d for d in EXTRACT_DIR.iterdir() if d.is_dir()])\n",
    "    available_years = [d.name for d in year_dirs]\n",
    "    \n",
    "    print(f\"\\nüìÖ Years Available: {', '.join(available_years) if available_years else 'None - download in progress'}\")\n",
    "    \n",
    "    # Analyze each year\n",
    "    for year_dir in year_dirs:\n",
    "        year = year_dir.name\n",
    "        csv_files = list(year_dir.glob('*.csv'))\n",
    "        total_size = sum(f.stat().st_size for f in csv_files) / (1024**3)  # GB\n",
    "        \n",
    "        # Group by table type\n",
    "        tables = {}\n",
    "        for f in csv_files:\n",
    "            table_name = f.name.split('_')[0] if '_' in f.name else f.stem\n",
    "            if table_name not in tables:\n",
    "                tables[table_name] = []\n",
    "            tables[table_name].append(f)\n",
    "        \n",
    "        year_info[year] = {\n",
    "            'files': len(csv_files),\n",
    "            'size_gb': total_size,\n",
    "            'tables': list(tables.keys())\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n   üìÅ Year {year}:\")\n",
    "        print(f\"      Files: {len(csv_files)}\")\n",
    "        print(f\"      Size: {total_size:.2f} GB\")\n",
    "        print(f\"      Tables: {', '.join(tables.keys())}\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Original data not yet extracted. Run download_original_data.py first.\")\n",
    "\n",
    "# Check GitHub repo for additional context\n",
    "if REPO_DIR.exists():\n",
    "    print(f\"\\nüìö GitHub Repository: ‚úÖ Available at {REPO_DIR}\")\n",
    "    # Check for useful scripts\n",
    "    scripts_dir = REPO_DIR / 'scripts'\n",
    "    if scripts_dir.exists():\n",
    "        py_files = list(scripts_dir.rglob('*.py'))\n",
    "        print(f\"   Processing scripts found: {len(py_files)}\")\n",
    "else:\n",
    "    print(f\"\\nüìö GitHub Repository: ‚ùå Not cloned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç ORIGINAL DATA STRUCTURE ANALYSIS (2016 Sample)\n",
      "================================================================================\n",
      "\n",
      "üìä tblSCTurbine:\n",
      "   Shape: (1000, 85)\n",
      "   Columns: 85\n",
      "   Turbines in sample: [np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6), np.int64(7), np.int64(8), np.int64(9), np.int64(10)]...\n",
      "   üéØ Turbine 6 (StationId 2304515): ‚úÖ Present\n",
      "   Key columns available:\n",
      "     ‚úì wtc_AcWindSp_mean\n",
      "     ‚úì wtc_ScYawPos_mean\n",
      "     ‚úì wtc_NacelPos_mean\n",
      "     ‚úì wtc_GenRpm_mean\n",
      "     ‚úì wtc_PitcPosA_mean\n",
      "     ‚úì wtc_PitcPosB_mean\n",
      "     ‚úì wtc_PitcPosC_mean\n",
      "\n",
      "üìä tblSCTurGrid:\n",
      "   Shape: (1000, 52)\n",
      "   Columns: 52\n",
      "   Turbines in sample: [np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6), np.int64(7), np.int64(8), np.int64(9), np.int64(10)]...\n",
      "   üéØ Turbine 6 (StationId 2304515): ‚úÖ Present\n",
      "   ‚úÖ Contains active power (target variable source)\n",
      "\n",
      "üìä tblSCTurTemp:\n",
      "   Shape: (1000, 142)\n",
      "   Columns: 142\n",
      "   Turbines in sample: [np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6), np.int64(7), np.int64(8), np.int64(9), np.int64(10)]...\n",
      "   üéØ Turbine 6 (StationId 2304515): ‚úÖ Present\n",
      "\n",
      "üìä tblSCTurFlag:\n",
      "   Shape: (1000, 46)\n",
      "   Columns: 46\n",
      "   Turbines in sample: [np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6), np.int64(7), np.int64(8), np.int64(9), np.int64(10)]...\n",
      "   üéØ Turbine 6 (StationId 2304515): ‚úÖ Present\n"
     ]
    }
   ],
   "source": [
    "# Deep dive into data structure if available\n",
    "if available_years and '2016' in available_years:\n",
    "    print(\"üîç ORIGINAL DATA STRUCTURE ANALYSIS (2016 Sample)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    year_dir = EXTRACT_DIR / '2016'\n",
    "    \n",
    "    # Analyze each table type\n",
    "    table_samples = {}\n",
    "    \n",
    "    for table_type in ['tblSCTurbine', 'tblSCTurGrid', 'tblSCTurTemp', 'tblSCTurFlag']:\n",
    "        files = list(year_dir.glob(f'{table_type}*.csv'))\n",
    "        if files:\n",
    "            print(f\"\\nüìä {table_type}:\")\n",
    "            # Read sample\n",
    "            df_sample = pd.read_csv(files[0], nrows=1000)\n",
    "            table_samples[table_type] = df_sample\n",
    "            \n",
    "            print(f\"   Shape: {df_sample.shape}\")\n",
    "            print(f\"   Columns: {len(df_sample.columns)}\")\n",
    "            \n",
    "            # Check for StationId (turbine identifier)\n",
    "            if 'StationId' in df_sample.columns:\n",
    "                unique_stations = df_sample['StationId'].unique()\n",
    "                turbine_ids = sorted([sid - 2304509 for sid in unique_stations])\n",
    "                print(f\"   Turbines in sample: {turbine_ids[:10]}...\" if len(turbine_ids) > 10 else f\"   Turbines: {turbine_ids}\")\n",
    "                \n",
    "                # Check for Turbine 6\n",
    "                turbine_6_station = 2304515\n",
    "                has_turbine_6 = turbine_6_station in unique_stations\n",
    "                print(f\"   üéØ Turbine 6 (StationId {turbine_6_station}): {'‚úÖ Present' if has_turbine_6 else '‚ùå Not in sample'}\")\n",
    "            \n",
    "            # Show key columns\n",
    "            if table_type == 'tblSCTurbine':\n",
    "                key_cols = ['wtc_AcWindSp_mean', 'wtc_ScYawPos_mean', 'wtc_NacelPos_mean', \n",
    "                           'wtc_GenRpm_mean', 'wtc_PitcPosA_mean', 'wtc_PitcPosB_mean', 'wtc_PitcPosC_mean']\n",
    "                print(\"   Key columns available:\")\n",
    "                for col in key_cols:\n",
    "                    if col in df_sample.columns:\n",
    "                        print(f\"     ‚úì {col}\")\n",
    "                    else:\n",
    "                        print(f\"     ‚úó {col}\")\n",
    "            \n",
    "            elif table_type == 'tblSCTurGrid':\n",
    "                if 'wtc_ActPower_mean' in df_sample.columns:\n",
    "                    print(f\"   ‚úÖ Contains active power (target variable source)\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Cannot analyze structure - data not yet available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä FEATURE COMPARISON\n",
      "================================================================================\n",
      "\n",
      "üèÜ Competition Dataset Features:\n",
      "   Total unique features per turbine: 29\n",
      "\n",
      "   Feature Categories:\n",
      "     SCADA: 28 features\n",
      "     Weather (ERA5): 0 features\n",
      "     Operational: 1 features\n",
      "\n",
      "üì¶ Original Dataset Features:\n",
      "   Total unique features: 317\n",
      "\n",
      "   Feature Overlap:\n",
      "     Matching features: 28/28\n",
      "     Coverage: 100.0%\n",
      "     Additional original features: 289\n",
      "\n",
      "   Unique to Original (sample):\n",
      "      1. wtc_PwrRedLowSpeed_stddev\n",
      "      2. wtc_PitcPosB_max\n",
      "      3. wtc_AmpPhT_mean\n",
      "      4. wtc_DeltaTmp_max\n",
      "      5. wtc_GenBeRTm_mean\n",
      "      6. wtc_AmpPhR_min\n",
      "      7. wtc_AmbieTmp_max\n",
      "      8. wtc_GridFreq_stddev\n",
      "      9. wtc_GFilB2Tm_max\n",
      "     10. wtc_ReactPwr_mean\n",
      "     ... and 279 more\n"
     ]
    }
   ],
   "source": [
    "print(\"üìä FEATURE COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Competition features (excluding turbine ID)\n",
    "comp_features = [col.split(';')[0] for col in train_df.columns if ';1' in col]\n",
    "comp_features_unique = list(dict.fromkeys(comp_features))  # Preserve order, remove duplicates\n",
    "\n",
    "print(f\"\\nüèÜ Competition Dataset Features:\")\n",
    "print(f\"   Total unique features per turbine: {len(comp_features_unique)}\")\n",
    "\n",
    "# Categorize features\n",
    "scada_features = [f for f in comp_features_unique if not f.startswith('ERA5_') and f != 'ShutdownDuration']\n",
    "weather_features = [f for f in comp_features_unique if f.startswith('ERA5_')]\n",
    "operational_features = ['ShutdownDuration'] if 'ShutdownDuration' in comp_features_unique else []\n",
    "\n",
    "print(f\"\\n   Feature Categories:\")\n",
    "print(f\"     SCADA: {len(scada_features)} features\")\n",
    "print(f\"     Weather (ERA5): {len(weather_features)} features\")\n",
    "print(f\"     Operational: {len(operational_features)} features\")\n",
    "\n",
    "# If original data is available, compare\n",
    "if available_years and 'table_samples' in locals():\n",
    "    print(f\"\\nüì¶ Original Dataset Features:\")\n",
    "    \n",
    "    all_original_cols = set()\n",
    "    for table_name, df in table_samples.items():\n",
    "        cols = set(df.columns) - {'TimeStamp', 'StationId'}\n",
    "        all_original_cols.update(cols)\n",
    "    \n",
    "    print(f\"   Total unique features: {len(all_original_cols)}\")\n",
    "    \n",
    "    # Find matching features\n",
    "    matches = []\n",
    "    for comp_feat in scada_features:\n",
    "        for orig_feat in all_original_cols:\n",
    "            if comp_feat in orig_feat or orig_feat in comp_feat:\n",
    "                matches.append((comp_feat, orig_feat))\n",
    "                break\n",
    "    \n",
    "    print(f\"\\n   Feature Overlap:\")\n",
    "    print(f\"     Matching features: {len(matches)}/{len(scada_features)}\")\n",
    "    print(f\"     Coverage: {len(matches)/len(scada_features)*100:.1f}%\")\n",
    "    print(f\"     Additional original features: {len(all_original_cols) - len(matches)}\")\n",
    "    \n",
    "    # Show unique original features\n",
    "    print(f\"\\n   Unique to Original (sample):\")\n",
    "    unique_original = all_original_cols - set([m[1] for m in matches])\n",
    "    for i, feat in enumerate(list(unique_original)[:10], 1):\n",
    "        print(f\"     {i:2}. {feat}\")\n",
    "    if len(unique_original) > 10:\n",
    "        print(f\"     ... and {len(unique_original)-10} more\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Processing Complexity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öôÔ∏è DATA PROCESSING COMPLEXITY ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "üìã Processing Steps Required:\n",
      "Step                                Complexity   Risk       Hours   \n",
      "----------------------------------------------------------------------\n",
      "StationId to Turbine Mapping        Low          üü¢ Low      0.5\n",
      "Multi-Table Joining                 High         üü° Medium   3.0\n",
      "Timestamp Alignment                 Medium       üü° Medium   2.0\n",
      "Wide Format Reshaping               Medium       üü¢ Low      1.5\n",
      "ERA5 Weather Integration            High         üî¥ High     4.0\n",
      "Validity Flag Calculation           Medium       üî¥ High     2.0\n",
      "Deduplication                       Low          üü¢ Low      0.5\n",
      "Feature Name Alignment              Medium       üü° Medium   1.5\n",
      "Data Validation                     High         üî¥ Critical 2.0\n",
      "----------------------------------------------------------------------\n",
      "TOTAL                                                       17.0\n",
      "\n",
      "üìä Complexity Summary:\n",
      "   Total estimated effort: 17.0 hours (~2.1 days)\n",
      "   High complexity steps: 3/9\n",
      "   High/Critical risk steps: 3/9\n",
      "\n",
      "   ‚ö†Ô∏è Risk factors:\n",
      "      - Data leakage from 2020 test period\n",
      "      - ERA5 API rate limits and download time\n",
      "      - Exact replication of competition preprocessing\n",
      "      - Memory constraints with 5 years of raw data\n"
     ]
    }
   ],
   "source": [
    "print(\"‚öôÔ∏è DATA PROCESSING COMPLEXITY ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Define processing steps with complexity scores\n",
    "processing_steps = [\n",
    "    {\n",
    "        \"step\": \"StationId to Turbine Mapping\",\n",
    "        \"description\": \"Map StationId (2304510-2304530) to turbine numbers (1-21)\",\n",
    "        \"complexity\": \"Low\",\n",
    "        \"effort_hours\": 0.5,\n",
    "        \"risk\": \"Low\",\n",
    "        \"code\": \"turbine_id = station_id - 2304509\"\n",
    "    },\n",
    "    {\n",
    "        \"step\": \"Multi-Table Joining\",\n",
    "        \"description\": \"Join tblSCTurbine, tblSCTurGrid, tblSCTurTemp, tblSCTurFlag\",\n",
    "        \"complexity\": \"High\",\n",
    "        \"effort_hours\": 3,\n",
    "        \"risk\": \"Medium\",\n",
    "        \"code\": \"Complex merge on TimeStamp + StationId with different schemas\"\n",
    "    },\n",
    "    {\n",
    "        \"step\": \"Timestamp Alignment\",\n",
    "        \"description\": \"Convert to UTC, align to 10-minute intervals, handle DST\",\n",
    "        \"complexity\": \"Medium\",\n",
    "        \"effort_hours\": 2,\n",
    "        \"risk\": \"Medium\",\n",
    "        \"code\": \"Resample to 10min, handle missing intervals\"\n",
    "    },\n",
    "    {\n",
    "        \"step\": \"Wide Format Reshaping\",\n",
    "        \"description\": \"Pivot from long format to wide (turbine;field structure)\",\n",
    "        \"complexity\": \"Medium\",\n",
    "        \"effort_hours\": 1.5,\n",
    "        \"risk\": \"Low\",\n",
    "        \"code\": \"pivot(index='timestamp', columns='turbine', values=fields)\"\n",
    "    },\n",
    "    {\n",
    "        \"step\": \"ERA5 Weather Integration\",\n",
    "        \"description\": \"Download ERA5 data for exact location, merge with SCADA\",\n",
    "        \"complexity\": \"High\",\n",
    "        \"effort_hours\": 4,\n",
    "        \"risk\": \"High\",\n",
    "        \"code\": \"API calls to ERA5, coordinate matching, temporal alignment\"\n",
    "    },\n",
    "    {\n",
    "        \"step\": \"Validity Flag Calculation\",\n",
    "        \"description\": \"Calculate is_valid from shutdown duration and operation time\",\n",
    "        \"complexity\": \"Medium\",\n",
    "        \"effort_hours\": 2,\n",
    "        \"risk\": \"High\",\n",
    "        \"code\": \"Complex business logic, must match competition exactly\"\n",
    "    },\n",
    "    {\n",
    "        \"step\": \"Deduplication\",\n",
    "        \"description\": \"Remove duplicate timestamps per turbine\",\n",
    "        \"complexity\": \"Low\",\n",
    "        \"effort_hours\": 0.5,\n",
    "        \"risk\": \"Low\",\n",
    "        \"code\": \"drop_duplicates(['timestamp', 'turbine'])\"\n",
    "    },\n",
    "    {\n",
    "        \"step\": \"Feature Name Alignment\",\n",
    "        \"description\": \"Map original column names to competition format\",\n",
    "        \"complexity\": \"Medium\",\n",
    "        \"effort_hours\": 1.5,\n",
    "        \"risk\": \"Medium\",\n",
    "        \"code\": \"Create mapping dictionary, validate consistency\"\n",
    "    },\n",
    "    {\n",
    "        \"step\": \"Data Validation\",\n",
    "        \"description\": \"Ensure no 2020 T1 data leakage, validate ranges\",\n",
    "        \"complexity\": \"High\",\n",
    "        \"effort_hours\": 2,\n",
    "        \"risk\": \"Critical\",\n",
    "        \"code\": \"Extensive validation, leakage detection\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Calculate totals\n",
    "total_effort = sum(step['effort_hours'] for step in processing_steps)\n",
    "high_complexity = sum(1 for step in processing_steps if step['complexity'] == 'High')\n",
    "high_risk = sum(1 for step in processing_steps if step['risk'] in ['High', 'Critical'])\n",
    "\n",
    "print(f\"\\nüìã Processing Steps Required:\")\n",
    "print(f\"{'Step':<35} {'Complexity':<12} {'Risk':<10} {'Hours':<8}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for step in processing_steps:\n",
    "    risk_emoji = \"üî¥\" if step['risk'] in ['High', 'Critical'] else \"üü°\" if step['risk'] == 'Medium' else \"üü¢\"\n",
    "    print(f\"{step['step']:<35} {step['complexity']:<12} {risk_emoji} {step['risk']:<8} {step['effort_hours']:.1f}\")\n",
    "\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'TOTAL':<35} {'':<12} {'':<10} {total_effort:.1f}\")\n",
    "\n",
    "print(f\"\\nüìä Complexity Summary:\")\n",
    "print(f\"   Total estimated effort: {total_effort:.1f} hours (~{total_effort/8:.1f} days)\")\n",
    "print(f\"   High complexity steps: {high_complexity}/{len(processing_steps)}\")\n",
    "print(f\"   High/Critical risk steps: {high_risk}/{len(processing_steps)}\")\n",
    "print(f\"\\n   ‚ö†Ô∏è Risk factors:\")\n",
    "print(f\"      - Data leakage from 2020 test period\")\n",
    "print(f\"      - ERA5 API rate limits and download time\")\n",
    "print(f\"      - Exact replication of competition preprocessing\")\n",
    "print(f\"      - Memory constraints with 5 years of raw data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Turbine 6 Specific Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ TURBINE 6 EXTRACTION FEASIBILITY\n",
      "================================================================================\n",
      "\n",
      "üå¨Ô∏è Why Turbine 6 Matters:\n",
      "   ‚Ä¢ Fills spatial gap between Turbines 5 and 7\n",
      "   ‚Ä¢ Enables better wake effect modeling\n",
      "   ‚Ä¢ Improves interpolation for missing data\n",
      "   ‚Ä¢ Provides additional correlation patterns\n",
      "\n",
      "‚úÖ Extraction Benefits:\n",
      "   üî¥ Spatial interpolation     Direct neighbor relationships\n",
      "   üî¥ Wake effect modeling      Critical for downstream turbine\n",
      "   üü° Feature engineering       Additional lag features\n",
      "   üü° Missing data handling     Better imputation accuracy\n",
      "\n",
      "‚ùå Extraction Challenges:\n",
      "   ‚Ä¢ Multi-table joins    4 tables with different schemas\n",
      "   ‚Ä¢ Data volume          ~8GB raw data to process\n",
      "   ‚Ä¢ Validation           Must match competition format exactly\n",
      "   ‚Ä¢ Testing              No ground truth for T6 validation\n",
      "\n",
      "üíª Extraction Approach (Simplified):\n",
      "\n",
      "```python\n",
      "# Step 1: Extract T6 from each table\n",
      "T6_STATION_ID = 2304515\n",
      "\n",
      "for year in [2016, 2017, 2018, 2019]:  # NOT 2020!\n",
      "    # Read and filter each table\n",
      "    turbine_df = pd.read_csv(f'tblSCTurbine_{year}.csv')\n",
      "    t6_turbine = turbine_df[turbine_df['StationId'] == T6_STATION_ID]\n",
      "\n",
      "    grid_df = pd.read_csv(f'tblSCTurGrid_{year}.csv')\n",
      "    t6_grid = grid_df[grid_df['StationId'] == T6_STATION_ID]\n",
      "\n",
      "    # Join tables\n",
      "    t6_data = t6_turbine.merge(t6_grid, on=['TimeStamp', 'StationId'])\n",
      "\n",
      "    # Reshape to competition format\n",
      "    t6_data = reshape_to_wide(t6_data, turbine_id=6)\n",
      "\n",
      "    # Merge with competition data\n",
      "    train_with_t6 = train_df.merge(t6_data, on='TimeStamp_StartFormat')\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"üéØ TURBINE 6 EXTRACTION FEASIBILITY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nüå¨Ô∏è Why Turbine 6 Matters:\")\n",
    "print(\"   ‚Ä¢ Fills spatial gap between Turbines 5 and 7\")\n",
    "print(\"   ‚Ä¢ Enables better wake effect modeling\")\n",
    "print(\"   ‚Ä¢ Improves interpolation for missing data\")\n",
    "print(\"   ‚Ä¢ Provides additional correlation patterns\")\n",
    "\n",
    "# If we have the repo, check turbine layout\n",
    "metadata_file = DATA_DIR / 'turbine_metadata.csv'\n",
    "if metadata_file.exists():\n",
    "    metadata = pd.read_csv(metadata_file)\n",
    "    print(\"\\nüìç Spatial Configuration:\")\n",
    "    for tid in [5, 6, 7]:\n",
    "        if tid in metadata['TurbineId'].values:\n",
    "            row = metadata[metadata['TurbineId'] == tid].iloc[0]\n",
    "            print(f\"   Turbine {tid}: Lat={row['Latitude']:.6f}, Lon={row['Longitude']:.6f}\")\n",
    "\n",
    "print(\"\\n‚úÖ Extraction Benefits:\")\n",
    "benefits = [\n",
    "    (\"Spatial interpolation\", \"High\", \"Direct neighbor relationships\"),\n",
    "    (\"Wake effect modeling\", \"High\", \"Critical for downstream turbine\"),\n",
    "    (\"Feature engineering\", \"Medium\", \"Additional lag features\"),\n",
    "    (\"Missing data handling\", \"Medium\", \"Better imputation accuracy\"),\n",
    "]\n",
    "\n",
    "for benefit, impact, description in benefits:\n",
    "    emoji = \"üî¥\" if impact == \"High\" else \"üü°\"\n",
    "    print(f\"   {emoji} {benefit:<25} {description}\")\n",
    "\n",
    "print(\"\\n‚ùå Extraction Challenges:\")\n",
    "challenges = [\n",
    "    (\"Multi-table joins\", \"4 tables with different schemas\"),\n",
    "    (\"Data volume\", \"~8GB raw data to process\"),\n",
    "    (\"Validation\", \"Must match competition format exactly\"),\n",
    "    (\"Testing\", \"No ground truth for T6 validation\"),\n",
    "]\n",
    "\n",
    "for challenge, description in challenges:\n",
    "    print(f\"   ‚Ä¢ {challenge:<20} {description}\")\n",
    "\n",
    "# Simplified extraction code\n",
    "print(\"\\nüíª Extraction Approach (Simplified):\")\n",
    "print(\"\"\"\n",
    "```python\n",
    "# Step 1: Extract T6 from each table\n",
    "T6_STATION_ID = 2304515\n",
    "\n",
    "for year in [2016, 2017, 2018, 2019]:  # NOT 2020!\n",
    "    # Read and filter each table\n",
    "    turbine_df = pd.read_csv(f'tblSCTurbine_{year}.csv')\n",
    "    t6_turbine = turbine_df[turbine_df['StationId'] == T6_STATION_ID]\n",
    "    \n",
    "    grid_df = pd.read_csv(f'tblSCTurGrid_{year}.csv')\n",
    "    t6_grid = grid_df[grid_df['StationId'] == T6_STATION_ID]\n",
    "    \n",
    "    # Join tables\n",
    "    t6_data = t6_turbine.merge(t6_grid, on=['TimeStamp', 'StationId'])\n",
    "    \n",
    "    # Reshape to competition format\n",
    "    t6_data = reshape_to_wide(t6_data, turbine_id=6)\n",
    "    \n",
    "    # Merge with competition data\n",
    "    train_with_t6 = train_df.merge(t6_data, on='TimeStamp_StartFormat')\n",
    "```\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Data Leakage Risk Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è DATA LEAKAGE RISK ASSESSMENT\n",
      "================================================================================\n",
      "\n",
      "üî¥ CRITICAL: 2020 Test Period Considerations\n",
      "--------------------------------------------------\n",
      "\n",
      "Data Type                      Status       Reason\n",
      "--------------------------------------------------------------------------------\n",
      "üö´ Turbine 1 Power (2020)       FORBIDDEN    Target variable - direct leakage\n",
      "üö´ Turbine 1 Features (2020)    FORBIDDEN    Would reveal target patterns\n",
      "‚úÖ Turbines 2-7 Power (2020)    SAFE         Not target, can use for features\n",
      "‚úÖ Turbines 2-7 Features (2020) SAFE         Available in test set\n",
      "‚úÖ Turbine 6 All Data (2020)    SAFE         Not in competition, no leakage\n",
      "‚úÖ Weather Data (2020)          SAFE         Already in test set\n",
      "‚úÖ All Data (2016-2019)         SAFE         Training period\n",
      "\n",
      "üìã Safe Implementation Checklist:\n",
      "   1. Filter out ALL Turbine 1 data from 2020\n",
      "   2. Verify date ranges after every merge\n",
      "   3. Create separate train/test pipelines\n",
      "   4. Add assertions to catch leakage\n",
      "   5. Log all data sources and filters\n",
      "   6. Test with known holdout period\n",
      "\n",
      "üí° Recommended Approach:\n",
      "\n",
      "1. TRAINING DATA (2016-2019):\n",
      "   - Use all turbines freely\n",
      "   - Extract Turbine 6 for full period\n",
      "\n",
      "2. TEST FEATURES (2020):\n",
      "   - Use Turbines 2,3,4,5,7 (already in test set)\n",
      "   - Add Turbine 6 if extracted\n",
      "   - NEVER touch Turbine 1 data\n",
      "\n",
      "3. VALIDATION:\n",
      "   - Create 2019 holdout to simulate test conditions\n",
      "   - Verify no information from future periods\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"‚ö†Ô∏è DATA LEAKAGE RISK ASSESSMENT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nüî¥ CRITICAL: 2020 Test Period Considerations\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Define safe and unsafe data\n",
    "leakage_matrix = [\n",
    "    (\"Turbine 1 Power (2020)\", \"FORBIDDEN\", \"Target variable - direct leakage\"),\n",
    "    (\"Turbine 1 Features (2020)\", \"FORBIDDEN\", \"Would reveal target patterns\"),\n",
    "    (\"Turbines 2-7 Power (2020)\", \"SAFE\", \"Not target, can use for features\"),\n",
    "    (\"Turbines 2-7 Features (2020)\", \"SAFE\", \"Available in test set\"),\n",
    "    (\"Turbine 6 All Data (2020)\", \"SAFE\", \"Not in competition, no leakage\"),\n",
    "    (\"Weather Data (2020)\", \"SAFE\", \"Already in test set\"),\n",
    "    (\"All Data (2016-2019)\", \"SAFE\", \"Training period\"),\n",
    "]\n",
    "\n",
    "print(f\"\\n{'Data Type':<30} {'Status':<12} {'Reason'}\")\n",
    "print(\"-\" * 80)\n",
    "for data_type, status, reason in leakage_matrix:\n",
    "    emoji = \"üö´\" if status == \"FORBIDDEN\" else \"‚úÖ\"\n",
    "    print(f\"{emoji} {data_type:<28} {status:<12} {reason}\")\n",
    "\n",
    "print(\"\\nüìã Safe Implementation Checklist:\")\n",
    "checklist = [\n",
    "    \"Filter out ALL Turbine 1 data from 2020\",\n",
    "    \"Verify date ranges after every merge\",\n",
    "    \"Create separate train/test pipelines\",\n",
    "    \"Add assertions to catch leakage\",\n",
    "    \"Log all data sources and filters\",\n",
    "    \"Test with known holdout period\",\n",
    "]\n",
    "\n",
    "for i, item in enumerate(checklist, 1):\n",
    "    print(f\"   {i}. {item}\")\n",
    "\n",
    "print(\"\\nüí° Recommended Approach:\")\n",
    "print(\"\"\"\n",
    "1. TRAINING DATA (2016-2019):\n",
    "   - Use all turbines freely\n",
    "   - Extract Turbine 6 for full period\n",
    "   \n",
    "2. TEST FEATURES (2020):\n",
    "   - Use Turbines 2,3,4,5,7 (already in test set)\n",
    "   - Add Turbine 6 if extracted\n",
    "   - NEVER touch Turbine 1 data\n",
    "   \n",
    "3. VALIDATION:\n",
    "   - Create 2019 holdout to simulate test conditions\n",
    "   - Verify no information from future periods\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Cost-Benefit Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üí∞ COMPREHENSIVE COST-BENEFIT ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "üìà BENEFITS:\n",
      "------------------------------------------------------------\n",
      "Turbine 6 Data       ‚ñà‚ñà‚ñà‚ñà‚ñà 5/5\n",
      "                     Critical gap - enables spatial modeling\n",
      "\n",
      "Additional Fields    ‚ñà‚ñà‚ñë‚ñë‚ñë 2/5\n",
      "                     96 vs 30 fields, but most redundant\n",
      "\n",
      "Raw Data Access      ‚ñà‚ñà‚ñà‚ñë‚ñë 3/5\n",
      "                     Full control over preprocessing\n",
      "\n",
      "All 21 Turbines      ‚ñà‚ñà‚ñë‚ñë‚ñë 2/5\n",
      "                     Limited value - too distant from T1\n",
      "\n",
      "Temporal Alignment   ‚ñà‚ñà‚ñà‚ñà‚ñë 4/5\n",
      "                     Exact match with competition period\n",
      "\n",
      "Total Benefit Score: 16/25\n",
      "\n",
      "üìâ COSTS/CHALLENGES:\n",
      "------------------------------------------------------------\n",
      "Processing Complexity ‚ñà‚ñà‚ñà‚ñà‚ñë 4/5\n",
      "                     ~16 hours effort, high risk\n",
      "\n",
      "Storage Requirements ‚ñà‚ñà‚ñà‚ñë‚ñë 3/5\n",
      "                     ~8GB raw + processed data\n",
      "\n",
      "Leakage Risk         ‚ñà‚ñà‚ñà‚ñà‚ñà 5/5\n",
      "                     Critical - could invalidate submission\n",
      "\n",
      "Validation Difficulty ‚ñà‚ñà‚ñà‚ñà‚ñë 4/5\n",
      "                     No ground truth for T6\n",
      "\n",
      "Time Investment      ‚ñà‚ñà‚ñà‚ñà‚ñë 4/5\n",
      "                     Could be spent on feature engineering\n",
      "\n",
      "Total Cost Score: 20/25\n",
      "\n",
      "‚öñÔ∏è NET ANALYSIS:\n",
      "============================================================\n",
      "Net Score: -4 (Benefits - Costs)\n",
      "Benefit/Cost Ratio: 0.80\n",
      "\n",
      "                      üéØ RECOMMENDATION                      \n",
      "============================================================\n",
      "             ‚ùå SKIP - Focus on competition data             \n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"üí∞ COMPREHENSIVE COST-BENEFIT ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Define scoring system\n",
    "def score_bar(score, max_score=5):\n",
    "    filled = \"‚ñà\" * score\n",
    "    empty = \"‚ñë\" * (max_score - score)\n",
    "    return f\"{filled}{empty}\"\n",
    "\n",
    "print(\"\\nüìà BENEFITS:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "benefits = [\n",
    "    (\"Turbine 6 Data\", 5, \"Critical gap - enables spatial modeling\"),\n",
    "    (\"Additional Fields\", 2, \"96 vs 30 fields, but most redundant\"),\n",
    "    (\"Raw Data Access\", 3, \"Full control over preprocessing\"),\n",
    "    (\"All 21 Turbines\", 2, \"Limited value - too distant from T1\"),\n",
    "    (\"Temporal Alignment\", 4, \"Exact match with competition period\"),\n",
    "]\n",
    "\n",
    "total_benefit = 0\n",
    "for item, score, description in benefits:\n",
    "    print(f\"{item:<20} {score_bar(score)} {score}/5\")\n",
    "    print(f\"{'':20} {description}\")\n",
    "    print()\n",
    "    total_benefit += score\n",
    "\n",
    "print(f\"Total Benefit Score: {total_benefit}/25\")\n",
    "\n",
    "print(\"\\nüìâ COSTS/CHALLENGES:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "costs = [\n",
    "    (\"Processing Complexity\", 4, \"~16 hours effort, high risk\"),\n",
    "    (\"Storage Requirements\", 3, \"~8GB raw + processed data\"),\n",
    "    (\"Leakage Risk\", 5, \"Critical - could invalidate submission\"),\n",
    "    (\"Validation Difficulty\", 4, \"No ground truth for T6\"),\n",
    "    (\"Time Investment\", 4, \"Could be spent on feature engineering\"),\n",
    "]\n",
    "\n",
    "total_cost = 0\n",
    "for item, score, description in costs:\n",
    "    print(f\"{item:<20} {score_bar(score)} {score}/5\")\n",
    "    print(f\"{'':20} {description}\")\n",
    "    print()\n",
    "    total_cost += score\n",
    "\n",
    "print(f\"Total Cost Score: {total_cost}/25\")\n",
    "\n",
    "# Net analysis\n",
    "print(\"\\n‚öñÔ∏è NET ANALYSIS:\")\n",
    "print(\"=\"*60)\n",
    "net_score = total_benefit - total_cost\n",
    "benefit_ratio = total_benefit / total_cost\n",
    "\n",
    "print(f\"Net Score: {net_score:+d} (Benefits - Costs)\")\n",
    "print(f\"Benefit/Cost Ratio: {benefit_ratio:.2f}\")\n",
    "\n",
    "if net_score > 0:\n",
    "    recommendation = \"‚úÖ PROCEED with Turbine 6 extraction\"\n",
    "    color = \"green\"\n",
    "elif net_score == 0:\n",
    "    recommendation = \"‚ö†Ô∏è MARGINAL - Only if time permits\"\n",
    "    color = \"yellow\"\n",
    "else:\n",
    "    recommendation = \"‚ùå SKIP - Focus on competition data\"\n",
    "    color = \"red\"\n",
    "\n",
    "print(f\"\\n{'üéØ RECOMMENDATION':^60}\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{recommendation:^60}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Strategic Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ STRATEGIC RECOMMENDATIONS\n",
      "================================================================================\n",
      "\n",
      "üìä DATA STRATEGY DECISION TREE:\n",
      "\n",
      "1. DO YOU HAVE A WORKING BASELINE?\n",
      "   ‚îú‚îÄ NO ‚Üí Focus 100% on competition data\n",
      "   ‚îÇ        Build baseline first\n",
      "   ‚îÇ\n",
      "   ‚îî‚îÄ YES ‚Üí 2. IS YOUR MAE < 200?\n",
      "            ‚îú‚îÄ NO ‚Üí Improve feature engineering\n",
      "            ‚îÇ        with competition data\n",
      "            ‚îÇ\n",
      "            ‚îî‚îÄ YES ‚Üí 3. DO YOU HAVE 2+ DAYS LEFT?\n",
      "                     ‚îú‚îÄ NO ‚Üí Polish existing model\n",
      "                     ‚îÇ\n",
      "                     ‚îî‚îÄ YES ‚Üí Consider T6 extraction\n",
      "\n",
      "\n",
      "üèÉ IMPLEMENTATION PHASES:\n",
      "============================================================\n",
      "\n",
      "Phase 1: Foundation (Days 1-3)\n",
      "  ‚Ä¢ Build baseline with competition data\n",
      "  ‚Ä¢ Implement proper cross-validation\n",
      "  ‚Ä¢ Create core feature engineering\n",
      "  ‚Ä¢ Establish evaluation pipeline\n",
      "\n",
      "Phase 2: Optimization (Days 4-5)\n",
      "  ‚Ä¢ Advanced feature engineering\n",
      "  ‚Ä¢ Model ensemble strategies\n",
      "  ‚Ä¢ Hyperparameter tuning\n",
      "  ‚Ä¢ Handle edge cases (shutdowns, nulls)\n",
      "\n",
      "Phase 3: Enhancement (Days 6-7)\n",
      "  ‚Ä¢ IF performance plateau: Extract T6\n",
      "  ‚Ä¢ Spatial correlation features\n",
      "  ‚Ä¢ Wake effect modeling\n",
      "  ‚Ä¢ Final ensemble refinement\n",
      "\n",
      "================================================================================\n",
      "üí° KEY INSIGHTS:\n",
      "\n",
      "1. Competition data is SUFFICIENT for strong performance\n",
      "2. Turbine 6 is NICE-TO-HAVE, not essential\n",
      "3. Feature engineering > More data\n",
      "4. Time is your most valuable resource\n",
      "5. Perfect is the enemy of good enough\n",
      "\n",
      "üéØ FINAL VERDICT:\n",
      "============================================================\n",
      "\n",
      "FOCUS ON COMPETITION DATA FIRST\n",
      "\n",
      "The original dataset offers marginal benefits that don't \n",
      "justify the implementation complexity and risk. \n",
      "\n",
      "Only consider Turbine 6 extraction if:\n",
      "  ‚úì You have a solid baseline (MAE < 250)\n",
      "  ‚úì You've exhausted feature engineering options\n",
      "  ‚úì You have 2+ days remaining\n",
      "  ‚úì You're comfortable with data pipeline complexity\n",
      "\n",
      "Remember: Many competitions are won with clever feature \n",
      "engineering on the provided data, not by adding more data.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"üéØ STRATEGIC RECOMMENDATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\"\"\n",
    "üìä DATA STRATEGY DECISION TREE:\n",
    "\n",
    "1. DO YOU HAVE A WORKING BASELINE?\n",
    "   ‚îú‚îÄ NO ‚Üí Focus 100% on competition data\n",
    "   ‚îÇ        Build baseline first\n",
    "   ‚îÇ\n",
    "   ‚îî‚îÄ YES ‚Üí 2. IS YOUR MAE < 200?\n",
    "            ‚îú‚îÄ NO ‚Üí Improve feature engineering\n",
    "            ‚îÇ        with competition data\n",
    "            ‚îÇ\n",
    "            ‚îî‚îÄ YES ‚Üí 3. DO YOU HAVE 2+ DAYS LEFT?\n",
    "                     ‚îú‚îÄ NO ‚Üí Polish existing model\n",
    "                     ‚îÇ\n",
    "                     ‚îî‚îÄ YES ‚Üí Consider T6 extraction\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "print(\"üèÉ IMPLEMENTATION PHASES:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "phases = [\n",
    "    (\n",
    "        \"Phase 1: Foundation (Days 1-3)\",\n",
    "        [\n",
    "            \"Build baseline with competition data\",\n",
    "            \"Implement proper cross-validation\",\n",
    "            \"Create core feature engineering\",\n",
    "            \"Establish evaluation pipeline\",\n",
    "        ]\n",
    "    ),\n",
    "    (\n",
    "        \"Phase 2: Optimization (Days 4-5)\",\n",
    "        [\n",
    "            \"Advanced feature engineering\",\n",
    "            \"Model ensemble strategies\",\n",
    "            \"Hyperparameter tuning\",\n",
    "            \"Handle edge cases (shutdowns, nulls)\",\n",
    "        ]\n",
    "    ),\n",
    "    (\n",
    "        \"Phase 3: Enhancement (Days 6-7)\",\n",
    "        [\n",
    "            \"IF performance plateau: Extract T6\",\n",
    "            \"Spatial correlation features\",\n",
    "            \"Wake effect modeling\",\n",
    "            \"Final ensemble refinement\",\n",
    "        ]\n",
    "    ),\n",
    "]\n",
    "\n",
    "for phase_name, tasks in phases:\n",
    "    print(f\"\\n{phase_name}\")\n",
    "    for task in tasks:\n",
    "        print(f\"  ‚Ä¢ {task}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üí° KEY INSIGHTS:\")\n",
    "print(\"\"\"\n",
    "1. Competition data is SUFFICIENT for strong performance\n",
    "2. Turbine 6 is NICE-TO-HAVE, not essential\n",
    "3. Feature engineering > More data\n",
    "4. Time is your most valuable resource\n",
    "5. Perfect is the enemy of good enough\n",
    "\"\"\")\n",
    "\n",
    "print(\"üéØ FINAL VERDICT:\")\n",
    "print(\"=\"*60)\n",
    "print(\"\"\"\n",
    "FOCUS ON COMPETITION DATA FIRST\n",
    "\n",
    "The original dataset offers marginal benefits that don't \n",
    "justify the implementation complexity and risk. \n",
    "\n",
    "Only consider Turbine 6 extraction if:\n",
    "  ‚úì You have a solid baseline (MAE < 250)\n",
    "  ‚úì You've exhausted feature engineering options\n",
    "  ‚úì You have 2+ days remaining\n",
    "  ‚úì You're comfortable with data pipeline complexity\n",
    "\n",
    "Remember: Many competitions are won with clever feature \n",
    "engineering on the provided data, not by adding more data.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Quick Reference Implementation Guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìö QUICK REFERENCE GUIDE\n",
      "================================================================================\n",
      "\n",
      "üîß IF YOU DECIDE TO EXTRACT TURBINE 6:\n",
      "\n",
      "```python\n",
      "# Constants\n",
      "TURBINE_6_STATION = 2304515\n",
      "SAFE_YEARS = [2016, 2017, 2018, 2019]  # NO 2020!\n",
      "\n",
      "# Step 1: Extract from each year\n",
      "t6_data_all = []\n",
      "for year in SAFE_YEARS:\n",
      "    t6_year = extract_turbine_6_year(year)\n",
      "    t6_data_all.append(t6_year)\n",
      "\n",
      "# Step 2: Combine and align\n",
      "t6_combined = pd.concat(t6_data_all)\n",
      "t6_combined = align_to_competition_format(t6_combined)\n",
      "\n",
      "# Step 3: Merge with training\n",
      "train_with_t6 = train_df.merge(\n",
      "    t6_combined, \n",
      "    on='TimeStamp_StartFormat',\n",
      "    how='left'\n",
      ")\n",
      "\n",
      "# Step 4: Validate no leakage\n",
      "assert train_with_t6[\n",
      "    train_with_t6['TimeStamp_StartFormat'] >= '2020-01-01'\n",
      "].empty, \"Data leakage detected!\"\n",
      "```\n",
      "\n",
      "üö´ NEVER DO THIS:\n",
      "```python\n",
      "# WRONG - Includes 2020 T1 data!\n",
      "all_data = pd.concat([data_2016, data_2017, data_2018, data_2019, data_2020])\n",
      "t1_data = all_data[all_data['turbine'] == 1]  # LEAKAGE!\n",
      "```\n",
      "\n",
      "‚úÖ CRITICAL CHECKS:\n",
      "```python\n",
      "# Always verify your data splits\n",
      "print(f\"Train max date: {train_data['timestamp'].max()}\")\n",
      "print(f\"Test min date: {test_data['timestamp'].min()}\")\n",
      "assert train_data['timestamp'].max() < test_data['timestamp'].min()\n",
      "```\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üìå SUMMARY: Use competition data. Consider T6 only if needed.\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"üìö QUICK REFERENCE GUIDE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\"\"\n",
    "üîß IF YOU DECIDE TO EXTRACT TURBINE 6:\n",
    "\n",
    "```python\n",
    "# Constants\n",
    "TURBINE_6_STATION = 2304515\n",
    "SAFE_YEARS = [2016, 2017, 2018, 2019]  # NO 2020!\n",
    "\n",
    "# Step 1: Extract from each year\n",
    "t6_data_all = []\n",
    "for year in SAFE_YEARS:\n",
    "    t6_year = extract_turbine_6_year(year)\n",
    "    t6_data_all.append(t6_year)\n",
    "\n",
    "# Step 2: Combine and align\n",
    "t6_combined = pd.concat(t6_data_all)\n",
    "t6_combined = align_to_competition_format(t6_combined)\n",
    "\n",
    "# Step 3: Merge with training\n",
    "train_with_t6 = train_df.merge(\n",
    "    t6_combined, \n",
    "    on='TimeStamp_StartFormat',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Step 4: Validate no leakage\n",
    "assert train_with_t6[\n",
    "    train_with_t6['TimeStamp_StartFormat'] >= '2020-01-01'\n",
    "].empty, \"Data leakage detected!\"\n",
    "```\n",
    "\n",
    "üö´ NEVER DO THIS:\n",
    "```python\n",
    "# WRONG - Includes 2020 T1 data!\n",
    "all_data = pd.concat([data_2016, data_2017, data_2018, data_2019, data_2020])\n",
    "t1_data = all_data[all_data['turbine'] == 1]  # LEAKAGE!\n",
    "```\n",
    "\n",
    "‚úÖ CRITICAL CHECKS:\n",
    "```python\n",
    "# Always verify your data splits\n",
    "print(f\"Train max date: {train_data['timestamp'].max()}\")\n",
    "print(f\"Test min date: {test_data['timestamp'].min()}\")\n",
    "assert train_data['timestamp'].max() < test_data['timestamp'].min()\n",
    "```\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìå SUMMARY: Use competition data. Consider T6 only if needed.\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Export Analysis Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ Analysis results saved to: /home/boujuan/Coding/hill-of-towie-wind-turbine/analysis/dataset_comparison_results.json\n",
      "\n",
      "‚úÖ Dataset comparison complete!\n",
      "\n",
      "üìå Next steps:\n",
      "   1. Focus on building strong baseline with competition data\n",
      "   2. Implement comprehensive feature engineering\n",
      "   3. Only consider T6 extraction if performance plateaus\n"
     ]
    }
   ],
   "source": [
    "# Save analysis results\n",
    "analysis_results = {\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "    \"recommendation\": \"Focus on competition data, consider T6 only if needed\",\n",
    "    \"competition_data\": {\n",
    "        \"turbines\": competition_turbines if 'competition_turbines' in locals() else [],\n",
    "        \"years\": \"2016-2020\",\n",
    "        \"features_per_turbine\": len(comp_features_unique) if 'comp_features_unique' in locals() else 0,\n",
    "        \"total_rows_train\": train_df.shape[0],\n",
    "        \"total_rows_test\": test_df.shape[0]\n",
    "    },\n",
    "    \"original_data\": {\n",
    "        \"years_available\": available_years,\n",
    "        \"has_turbine_6\": True,  # Confirmed in analysis\n",
    "        \"total_turbines\": 21,\n",
    "        \"processing_effort_hours\": total_effort if 'total_effort' in locals() else 16\n",
    "    },\n",
    "    \"cost_benefit\": {\n",
    "        \"benefit_score\": total_benefit if 'total_benefit' in locals() else 16,\n",
    "        \"cost_score\": total_cost if 'total_cost' in locals() else 20,\n",
    "        \"net_score\": net_score if 'net_score' in locals() else -4,\n",
    "        \"recommendation\": recommendation if 'recommendation' in locals() else \"Skip\"\n",
    "    },\n",
    "    \"critical_warnings\": [\n",
    "        \"Never use Turbine 1 data from 2020\",\n",
    "        \"Validate no data leakage\",\n",
    "        \"Focus on competition data first\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Save to JSON\n",
    "output_path = PROJECT_ROOT / 'analysis' / 'dataset_comparison_results.json'\n",
    "output_path.parent.mkdir(exist_ok=True)\n",
    "\n",
    "with open(output_path, 'w') as f:\n",
    "    json.dump(analysis_results, f, indent=2, default=str)\n",
    "\n",
    "print(f\"\\nüíæ Analysis results saved to: {output_path}\")\n",
    "print(\"\\n‚úÖ Dataset comparison complete!\")\n",
    "print(\"\\nüìå Next steps:\")\n",
    "print(\"   1. Focus on building strong baseline with competition data\")\n",
    "print(\"   2. Implement comprehensive feature engineering\")\n",
    "print(\"   3. Only consider T6 extraction if performance plateaus\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hot-kaggle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
