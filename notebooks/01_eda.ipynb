{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hill of Towie Wind Turbine Power Prediction - Exploratory Data Analysis\n",
    "\n",
    "This notebook contains exploratory data analysis for the wind turbine power prediction competition.\n",
    "\n",
    "## Goals:\n",
    "1. Load and examine the training and test datasets\n",
    "2. Understand the data structure and features\n",
    "3. Analyze distributions and patterns\n",
    "4. Identify correlations with target variable\n",
    "5. Explore temporal patterns if applicable\n",
    "6. Detect outliers and missing values\n",
    "7. Plan feature engineering strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Setup plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "# Add src to path for imports\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "from config import TRAIN_FILE, TEST_FILE, SAMPLE_SUBMISSION_FILE\n",
    "from utils import load_data, setup_logging\n",
    "\n",
    "# Setup logging\n",
    "logger = setup_logging(log_level=\"INFO\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "print(\"Loading datasets...\")\n",
    "train_df = load_data(TRAIN_FILE)\n",
    "test_df = load_data(TEST_FILE)\n",
    "sample_submission = load_data(SAMPLE_SUBMISSION_FILE)\n",
    "\n",
    "print(f\"Training data shape: {train_df.shape}\")\n",
    "print(f\"Test data shape: {test_df.shape}\")\n",
    "print(f\"Sample submission shape: {sample_submission.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic info about training data\n",
    "print(\"=== TRAINING DATA INFO ===\")\n",
    "print(train_df.info())\n",
    "print(\"\\n=== FIRST FEW ROWS ===\")\n",
    "display(train_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic info about test data\n",
    "print(\"=== TEST DATA INFO ===\")\n",
    "print(test_df.info())\n",
    "print(\"\\n=== FIRST FEW ROWS ===\")\n",
    "display(test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample submission format\n",
    "print(\"=== SAMPLE SUBMISSION ===\")\n",
    "display(sample_submission.head())\n",
    "print(f\"Submission columns: {sample_submission.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Statistical Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary of numerical features\n",
    "print(\"=== TRAINING DATA STATISTICS ===\")\n",
    "display(train_df.describe())\n",
    "\n",
    "print(\"\\n=== TEST DATA STATISTICS ===\")\n",
    "display(test_df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Missing Values Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"=== MISSING VALUES IN TRAINING DATA ===\")\n",
    "train_missing = train_df.isnull().sum()\n",
    "train_missing_pct = 100 * train_missing / len(train_df)\n",
    "missing_df = pd.DataFrame({\n",
    "    'Column': train_missing.index,\n",
    "    'Missing Count': train_missing.values,\n",
    "    'Missing %': train_missing_pct.values\n",
    "}).sort_values('Missing Count', ascending=False)\n",
    "display(missing_df[missing_df['Missing Count'] > 0])\n",
    "\n",
    "print(\"\\n=== MISSING VALUES IN TEST DATA ===\")\n",
    "test_missing = test_df.isnull().sum()\n",
    "test_missing_pct = 100 * test_missing / len(test_df)\n",
    "missing_test_df = pd.DataFrame({\n",
    "    'Column': test_missing.index,\n",
    "    'Missing Count': test_missing.values,\n",
    "    'Missing %': test_missing_pct.values\n",
    "}).sort_values('Missing Count', ascending=False)\n",
    "display(missing_test_df[missing_test_df['Missing Count'] > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Target Variable Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify target column (assuming it's the last column or has 'power' in name)\n",
    "potential_targets = [col for col in train_df.columns if 'power' in col.lower()]\n",
    "if not potential_targets:\n",
    "    # If no column with 'power', assume last numerical column is target\n",
    "    numerical_cols = train_df.select_dtypes(include=[np.number]).columns\n",
    "    target_col = numerical_cols[-1]\n",
    "else:\n",
    "    target_col = potential_targets[0]\n",
    "\n",
    "print(f\"Assumed target column: {target_col}\")\n",
    "\n",
    "if target_col in train_df.columns:\n",
    "    target = train_df[target_col]\n",
    "    \n",
    "    print(f\"Target statistics:\")\n",
    "    print(f\"  Mean: {target.mean():.2f}\")\n",
    "    print(f\"  Std: {target.std():.2f}\")\n",
    "    print(f\"  Min: {target.min():.2f}\")\n",
    "    print(f\"  Max: {target.max():.2f}\")\n",
    "    print(f\"  Median: {target.median():.2f}\")\n",
    "else:\n",
    "    print(\"Could not identify target column. Please update manually.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target distribution visualization\n",
    "if target_col in train_df.columns:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Histogram\n",
    "    axes[0, 0].hist(target, bins=50, alpha=0.7, color='blue')\n",
    "    axes[0, 0].set_title('Target Distribution')\n",
    "    axes[0, 0].set_xlabel(target_col)\n",
    "    axes[0, 0].set_ylabel('Frequency')\n",
    "    \n",
    "    # Box plot\n",
    "    axes[0, 1].boxplot(target)\n",
    "    axes[0, 1].set_title('Target Box Plot')\n",
    "    axes[0, 1].set_ylabel(target_col)\n",
    "    \n",
    "    # QQ plot\n",
    "    from scipy import stats\n",
    "    stats.probplot(target, dist=\"norm\", plot=axes[1, 0])\n",
    "    axes[1, 0].set_title('Target Q-Q Plot')\n",
    "    \n",
    "    # Time series plot (if we can identify a time column)\n",
    "    time_cols = [col for col in train_df.columns if any(word in col.lower() for word in ['time', 'date', 'timestamp'])]\n",
    "    if time_cols:\n",
    "        time_col = time_cols[0]\n",
    "        if pd.api.types.is_datetime64_any_dtype(train_df[time_col]) or train_df[time_col].dtype == 'object':\n",
    "            try:\n",
    "                time_data = pd.to_datetime(train_df[time_col])\n",
    "                axes[1, 1].plot(time_data, target, alpha=0.7)\n",
    "                axes[1, 1].set_title(f'Target vs {time_col}')\n",
    "                axes[1, 1].set_xlabel(time_col)\n",
    "                axes[1, 1].set_ylabel(target_col)\n",
    "            except:\n",
    "                axes[1, 1].scatter(range(len(target)), target, alpha=0.5)\n",
    "                axes[1, 1].set_title('Target vs Index')\n",
    "        else:\n",
    "            axes[1, 1].scatter(range(len(target)), target, alpha=0.5)\n",
    "            axes[1, 1].set_title('Target vs Index')\n",
    "    else:\n",
    "        axes[1, 1].scatter(range(len(target)), target, alpha=0.5)\n",
    "        axes[1, 1].set_title('Target vs Index')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze numerical features\n",
    "numerical_features = train_df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "if target_col in numerical_features:\n",
    "    numerical_features.remove(target_col)\n",
    "\n",
    "print(f\"Numerical features ({len(numerical_features)}): {numerical_features[:10]}...\")\n",
    "\n",
    "# Categorical features\n",
    "categorical_features = train_df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "print(f\"Categorical features ({len(categorical_features)}): {categorical_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation analysis with target\n",
    "if target_col in train_df.columns and len(numerical_features) > 0:\n",
    "    correlations = train_df[numerical_features + [target_col]].corr()[target_col].sort_values(key=abs, ascending=False)\n",
    "    correlations = correlations.drop(target_col)  # Remove self-correlation\n",
    "    \n",
    "    print(\"Top 10 features correlated with target:\")\n",
    "    display(correlations.head(10).to_frame('Correlation'))\n",
    "    \n",
    "    # Plot correlations\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    top_corr = correlations.head(15)\n",
    "    colors = ['red' if x < 0 else 'blue' for x in top_corr.values]\n",
    "    plt.barh(range(len(top_corr)), top_corr.values, color=colors, alpha=0.7)\n",
    "    plt.yticks(range(len(top_corr)), top_corr.index)\n",
    "    plt.xlabel('Correlation with Target')\n",
    "    plt.title('Feature Correlation with Target')\n",
    "    plt.grid(axis='x', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Feature Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distributions of top correlated features\n",
    "if target_col in train_df.columns and len(numerical_features) > 0:\n",
    "    top_features = correlations.head(8).index.tolist()\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    for i, feature in enumerate(top_features):\n",
    "        if feature in train_df.columns:\n",
    "            axes[i].hist(train_df[feature], bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "            axes[i].set_title(f'Distribution of {feature}')\n",
    "            axes[i].set_xlabel(feature)\n",
    "            axes[i].set_ylabel('Frequency')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix of top features\n",
    "if target_col in train_df.columns and len(numerical_features) > 0:\n",
    "    top_features = correlations.head(10).index.tolist() + [target_col]\n",
    "    correlation_matrix = train_df[top_features].corr()\n",
    "    \n",
    "    plt.figure(figsize=(12, 10))\n",
    "    mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "    sns.heatmap(correlation_matrix, mask=mask, annot=True, cmap='coolwarm', center=0,\n",
    "                square=True, linewidths=0.5, cbar_kws={\"shrink\": .8})\n",
    "    plt.title('Correlation Matrix of Top Features')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Outlier Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect outliers using IQR method for top features\n",
    "if len(numerical_features) > 0:\n",
    "    top_features_for_outliers = correlations.head(5).index.tolist()\n",
    "    \n",
    "    outlier_summary = []\n",
    "    \n",
    "    for feature in top_features_for_outliers:\n",
    "        if feature in train_df.columns:\n",
    "            Q1 = train_df[feature].quantile(0.25)\n",
    "            Q3 = train_df[feature].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            lower_bound = Q1 - 1.5 * IQR\n",
    "            upper_bound = Q3 + 1.5 * IQR\n",
    "            \n",
    "            outliers = train_df[(train_df[feature] < lower_bound) | (train_df[feature] > upper_bound)]\n",
    "            outlier_count = len(outliers)\n",
    "            outlier_pct = 100 * outlier_count / len(train_df)\n",
    "            \n",
    "            outlier_summary.append({\n",
    "                'Feature': feature,\n",
    "                'Outlier Count': outlier_count,\n",
    "                'Outlier %': outlier_pct,\n",
    "                'Lower Bound': lower_bound,\n",
    "                'Upper Bound': upper_bound\n",
    "            })\n",
    "    \n",
    "    outlier_df = pd.DataFrame(outlier_summary)\n",
    "    print(\"Outlier Analysis:\")\n",
    "    display(outlier_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Temporal Analysis (if applicable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time series analysis if we have datetime columns\n",
    "time_cols = [col for col in train_df.columns if any(word in col.lower() for word in ['time', 'date', 'timestamp'])]\n",
    "\n",
    "if time_cols and target_col in train_df.columns:\n",
    "    time_col = time_cols[0]\n",
    "    print(f\"Analyzing temporal patterns using column: {time_col}\")\n",
    "    \n",
    "    try:\n",
    "        # Convert to datetime\n",
    "        train_df_temp = train_df.copy()\n",
    "        train_df_temp[time_col] = pd.to_datetime(train_df_temp[time_col])\n",
    "        \n",
    "        # Extract time features\n",
    "        train_df_temp['hour'] = train_df_temp[time_col].dt.hour\n",
    "        train_df_temp['day_of_week'] = train_df_temp[time_col].dt.dayofweek\n",
    "        train_df_temp['month'] = train_df_temp[time_col].dt.month\n",
    "        \n",
    "        # Plot temporal patterns\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        \n",
    "        # Hourly pattern\n",
    "        hourly_avg = train_df_temp.groupby('hour')[target_col].mean()\n",
    "        axes[0, 0].plot(hourly_avg.index, hourly_avg.values, marker='o')\n",
    "        axes[0, 0].set_title('Average Target by Hour')\n",
    "        axes[0, 0].set_xlabel('Hour')\n",
    "        axes[0, 0].set_ylabel(f'Average {target_col}')\n",
    "        \n",
    "        # Daily pattern\n",
    "        daily_avg = train_df_temp.groupby('day_of_week')[target_col].mean()\n",
    "        day_names = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
    "        axes[0, 1].bar(range(7), daily_avg.values)\n",
    "        axes[0, 1].set_title('Average Target by Day of Week')\n",
    "        axes[0, 1].set_xlabel('Day of Week')\n",
    "        axes[0, 1].set_ylabel(f'Average {target_col}')\n",
    "        axes[0, 1].set_xticks(range(7))\n",
    "        axes[0, 1].set_xticklabels(day_names)\n",
    "        \n",
    "        # Monthly pattern\n",
    "        monthly_avg = train_df_temp.groupby('month')[target_col].mean()\n",
    "        axes[1, 0].plot(monthly_avg.index, monthly_avg.values, marker='o')\n",
    "        axes[1, 0].set_title('Average Target by Month')\n",
    "        axes[1, 0].set_xlabel('Month')\n",
    "        axes[1, 0].set_ylabel(f'Average {target_col}')\n",
    "        \n",
    "        # Time series plot (sampled if too many points)\n",
    "        if len(train_df_temp) > 10000:\n",
    "            sample_df = train_df_temp.sample(10000).sort_values(time_col)\n",
    "        else:\n",
    "            sample_df = train_df_temp.sort_values(time_col)\n",
    "        \n",
    "        axes[1, 1].plot(sample_df[time_col], sample_df[target_col], alpha=0.6)\n",
    "        axes[1, 1].set_title('Target Over Time (Sample)')\n",
    "        axes[1, 1].set_xlabel(time_col)\n",
    "        axes[1, 1].set_ylabel(target_col)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Could not perform temporal analysis: {e}\")\n",
    "else:\n",
    "    print(\"No datetime columns found for temporal analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Train vs Test Distribution Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare feature distributions between train and test\n",
    "common_features = list(set(train_df.columns) & set(test_df.columns))\n",
    "common_numerical = [col for col in common_features if train_df[col].dtype in ['int64', 'float64']]\n",
    "\n",
    "if len(common_numerical) > 0:\n",
    "    # Plot distributions for top features\n",
    "    top_common_features = common_numerical[:6]\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    for i, feature in enumerate(top_common_features):\n",
    "        if feature in train_df.columns and feature in test_df.columns:\n",
    "            axes[i].hist(train_df[feature], bins=30, alpha=0.7, label='Train', color='blue')\n",
    "            axes[i].hist(test_df[feature], bins=30, alpha=0.7, label='Test', color='red')\n",
    "            axes[i].set_title(f'Distribution Comparison: {feature}')\n",
    "            axes[i].set_xlabel(feature)\n",
    "            axes[i].set_ylabel('Frequency')\n",
    "            axes[i].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Key Insights and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of findings\n",
    "print(\"=== KEY INSIGHTS FROM EDA ===\")\n",
    "print(f\"1. Dataset shape - Train: {train_df.shape}, Test: {test_df.shape}\")\n",
    "print(f\"2. Target column identified: {target_col if 'target_col' in locals() else 'Not identified'}\")\n",
    "print(f\"3. Number of numerical features: {len(numerical_features)}\")\n",
    "print(f\"4. Number of categorical features: {len(categorical_features)}\")\n",
    "print(f\"5. Missing values present: {train_df.isnull().sum().sum() > 0}\")\n",
    "print(f\"6. Temporal features available: {len(time_cols) > 0 if 'time_cols' in locals() else False}\")\n",
    "\n",
    "if 'correlations' in locals():\n",
    "    print(f\"7. Top correlated feature: {correlations.index[0]} (correlation: {correlations.iloc[0]:.3f})\")\n",
    "\n",
    "print(\"\\n=== RECOMMENDED NEXT STEPS ===\")\n",
    "print(\"1. Feature Engineering:\")\n",
    "print(\"   - Create lag features if temporal data available\")\n",
    "print(\"   - Generate polynomial/interaction features for top correlated features\")\n",
    "print(\"   - Create statistical features (rolling means, std, etc.)\")\n",
    "print(\"   - Handle categorical variables with encoding\")\n",
    "print(\"\\n2. Data Preprocessing:\")\n",
    "print(\"   - Handle missing values appropriately\")\n",
    "print(\"   - Consider outlier treatment\")\n",
    "print(\"   - Scale/normalize features if needed\")\n",
    "print(\"\\n3. Model Development:\")\n",
    "print(\"   - Start with simple baseline models\")\n",
    "print(\"   - Try ensemble methods (XGBoost, LightGBM, CatBoost)\")\n",
    "print(\"   - Implement cross-validation strategy\")\n",
    "print(\"   - Consider neural networks for complex patterns\")\n",
    "print(\"\\n4. Validation Strategy:\")\n",
    "print(\"   - Time-based splits if temporal data\")\n",
    "print(\"   - Stratified splits for stable validation\")\n",
    "print(\"   - Monitor overfitting carefully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Save Processed Data (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally save cleaned/processed versions of the data\n",
    "# This cell can be uncommented and modified based on findings\n",
    "\n",
    "# from utils import save_data\n",
    "# from config import PROCESSED_DATA_DIR\n",
    "\n",
    "# # Save processed training data\n",
    "# save_data(train_df, PROCESSED_DATA_DIR / \"train_processed.parquet\")\n",
    "# save_data(test_df, PROCESSED_DATA_DIR / \"test_processed.parquet\")\n",
    "\n",
    "# print(\"Processed data saved successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hot-kaggle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
